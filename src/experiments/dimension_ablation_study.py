"""
4D ë””ì§€í„¸ í˜ë¡œëª¬ ì°¨ì›ë³„ ê¸°ì—¬ë„ ë¶„ì„ (í™•ì¥ëœ Ablation Study)

ê° ì°¨ì›ì˜ ê¸°ì—¬ë„ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ 4D í˜ë¡œëª¬ì˜ íš¨ê³¼ë¥¼ ì •ëŸ‰í™”í•©ë‹ˆë‹¤.
"""

import os
import sys
import yaml
import logging
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Any
from itertools import combinations
import argparse
from tqdm import tqdm
import ray
from scipy.stats import f_oneway, ttest_ind, chi2_contingency
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import warnings
warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.experiments.run_experiment import ExperimentRunner
from src.core.pheromone_vector import PheromoneVector
from src.utils.metrics import MetricsTracker

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DimensionAblationStudy:
    """4D í˜ë¡œëª¬ ì°¨ì›ë³„ ê¸°ì—¬ë„ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, config_path: str):
        """
        ì´ˆê¸°í™”
        
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
            
        self.results_dir = self.config['experiment']['log_dir']
        os.makedirs(self.results_dir, exist_ok=True)
        
        # ì‹¤í—˜ êµ¬ì„± ì •ë³´
        self.dimension_configs = self._prepare_dimension_configurations()
        self.all_results = {}
        self.analysis_results = {}
        
        # Ray ì´ˆê¸°í™”
        if not ray.is_initialized():
            try:
                ray_config = self.config.get('ray', {})
                # íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ê°€
                ray_config.setdefault('_temp_dir', None)  # ê¸°ë³¸ ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
                ray.init(**ray_config)
                print(f"Ray ì´ˆê¸°í™” ì„±ê³µ: {ray.cluster_resources()}")
            except Exception as e:
                print(f"Ray ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                print("Ray ì—†ì´ ì‹¤í–‰ì„ ì‹œë„í•©ë‹ˆë‹¤...")
                # Ray ì—†ì´ ì‹¤í–‰í•˜ë„ë¡ í”Œë˜ê·¸ ì„¤ì •
                self.use_ray = False
            else:
                self.use_ray = True
        else:
            self.use_ray = True
            
    def _prepare_dimension_configurations(self) -> Dict[str, Dict]:
        """ì°¨ì› êµ¬ì„± ì •ë³´ ì¤€ë¹„"""
        configs = {}
        
        analysis_config = self.config['dimension_analysis']
        
        # ë‹¨ì¼ ì°¨ì›
        for config_name, dimensions in analysis_config['single_dimensions'][0].items():
            configs[f"single_{config_name}"] = dimensions
            
        # 2ì°¨ì› ì¡°í•©
        for config_dict in analysis_config['two_dimensions']:
            for config_name, dimensions in config_dict.items():
                configs[f"two_{config_name}"] = dimensions
                
        # 3ì°¨ì› ì¡°í•©  
        for config_dict in analysis_config['three_dimensions']:
            for config_name, dimensions in config_dict.items():
                configs[f"three_{config_name}"] = dimensions
                
        # ì „ì²´ 4ì°¨ì›
        configs["full_4d"] = analysis_config['full_4d']
        
        logger.info(f"ì¤€ë¹„ëœ ì°¨ì› êµ¬ì„± ìˆ˜: {len(configs)}")
        return configs
    
    def run_single_experiment(self, dimension_config: Dict, config_name: str, run_id: int) -> Dict:
        """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰"""
        try:
            # ê¸°ë³¸ ì„¤ì • ë³µì‚¬
            experiment_config = self.config.copy()
            
            # í˜ë¡œëª¬ ì°¨ì› ì„¤ì • ì—…ë°ì´íŠ¸
            experiment_config['pheromone']['dimensions'] = dimension_config
            
            # ì‹¤í—˜ ì´ë¦„ ì—…ë°ì´íŠ¸
            experiment_config['experiment']['name'] = f"ablation_{config_name}_run_{run_id}"
            experiment_config['experiment']['log_dir'] = os.path.join(
                self.results_dir, config_name, f"run_{run_id}"
            )
            
            # ì‹œë“œ ì„¤ì •
            if 'random_seeds' in self.config['execution']:
                seed = self.config['execution']['random_seeds'][run_id % len(self.config['execution']['random_seeds'])]
                np.random.seed(seed)
                torch.manual_seed(seed)
            
            # ì„ì‹œ ì„¤ì • íŒŒì¼ ìƒì„±
            temp_config_path = f"temp_config_{config_name}_{run_id}.yaml"
            with open(temp_config_path, 'w', encoding='utf-8') as f:
                yaml.dump(experiment_config, f, allow_unicode=True, indent=2)
            
            try:
                # ì‹¤í—˜ ì‹¤í–‰
                experiment = ExperimentRunner(temp_config_path)
                results = experiment.run_experiment()
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                if os.path.exists(temp_config_path):
                    os.remove(temp_config_path)
            
            # ë©”íŠ¸ë¦­ ì¶”ì¶œ
            metrics_summary = experiment.metrics_tracker.get_summary()
            
            # ì°¨ì›ë³„ íŠ¹ì„± ë¶„ì„
            dimension_analysis = self._analyze_dimension_usage(
                experiment, dimension_config, metrics_summary
            )
            
            return {
                'config_name': config_name,
                'run_id': run_id,
                'dimension_config': dimension_config,
                'metrics': metrics_summary,
                'dimension_analysis': dimension_analysis,
                'success': True
            }
            
        except Exception as e:
            logger.error(f"ì‹¤í—˜ ì‹¤í–‰ ì˜¤ë¥˜ ({config_name}, run {run_id}): {e}")
            return {
                'config_name': config_name,
                'run_id': run_id,
                'dimension_config': dimension_config,
                'error': str(e),
                'success': False
            }
    
    def _analyze_dimension_usage(self, experiment: ExperimentRunner, 
                                dimension_config: Dict, metrics: Dict) -> Dict:
        """ì°¨ì› ì‚¬ìš©ëŸ‰ ë¶„ì„"""
        analysis = {
            'active_dimensions': [],
            'dimension_weights': {},
            'utilization_rates': {},
            'information_content': {},
        }
        
        # í™œì„± ì°¨ì› ì‹ë³„
        for dim_name, dim_size in dimension_config.items():
            if dim_size > 0:
                analysis['active_dimensions'].append(dim_name)
                analysis['dimension_weights'][dim_name] = dim_size
        
        # í˜ë¡œëª¬ í•„ë“œì—ì„œ ì°¨ì›ë³„ í™œìš©ë„ ê³„ì‚°
        if hasattr(experiment, 'pheromone_field') and experiment.pheromone_field.field:
            total_pheromones = list(experiment.pheromone_field.field.values())
            if total_pheromones:
                # í‰ê·  í˜ë¡œëª¬ ë²¡í„° ê³„ì‚°
                avg_vector = sum(total_pheromones, PheromoneVector.zeros(dimension_config))
                avg_vector = avg_vector / len(total_pheromones)
                
                # ì°¨ì›ë³„ í™œìš©ë¥ 
                dim_start = 0
                for dim_name, dim_size in dimension_config.items():
                    if dim_size > 0:
                        dim_values = avg_vector.to_array()[dim_start:dim_start + dim_size]
                        analysis['utilization_rates'][dim_name] = float(np.mean(np.abs(dim_values)))
                        analysis['information_content'][dim_name] = float(np.std(dim_values))
                        dim_start += dim_size
        
        return analysis
    
    def run_all_experiments(self) -> Dict:
        """ëª¨ë“  ì°¨ì› ì¡°í•©ì— ëŒ€í•´ ì‹¤í—˜ ì‹¤í–‰"""
        logger.info("ì°¨ì›ë³„ ì ˆì œ ì—°êµ¬ ì‹¤í—˜ ì‹œì‘")
        
        runs_per_config = self.config['execution']['runs_per_configuration']
        
        for config_name, dimension_config in tqdm(self.dimension_configs.items(), desc="ì°¨ì› êµ¬ì„±"):
            config_results = []
            
            for run_id in tqdm(range(runs_per_config), desc=f"{config_name} ì‹¤í–‰", leave=False):
                result = self.run_single_experiment(dimension_config, config_name, run_id)
                config_results.append(result)
                
                if not result['success']:
                    logger.warning(f"ì‹¤í—˜ ì‹¤íŒ¨: {config_name} run {run_id}")
            
            self.all_results[config_name] = config_results
            
            # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
            self._save_intermediate_results(config_name, config_results)
        
        logger.info("ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ")
        return self.all_results
    
    def _save_intermediate_results(self, config_name: str, results: List[Dict]):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        results_path = os.path.join(self.results_dir, f"{config_name}_results.yaml")
        
        # ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        serializable_results = []
        for result in results:
            if result['success']:
                serializable_result = {
                    'config_name': result['config_name'],
                    'run_id': result['run_id'],
                    'dimension_config': result['dimension_config'],
                    'dimension_analysis': result['dimension_analysis'],
                    'success': result['success']
                }
                # ë©”íŠ¸ë¦­ì—ì„œ ìˆ«ì ê°’ë§Œ ì¶”ì¶œ
                if 'metrics' in result:
                    serializable_result['key_metrics'] = self._extract_key_metrics(result['metrics'])
                
                serializable_results.append(serializable_result)
            else:
                serializable_results.append({
                    'config_name': result['config_name'],
                    'run_id': result['run_id'],
                    'error': result['error'],
                    'success': result['success']
                })
        
        with open(results_path, 'w', encoding='utf-8') as f:
            yaml.dump(serializable_results, f, allow_unicode=True, indent=2)
    
    def _extract_key_metrics(self, metrics: Dict) -> Dict:
        """ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
        key_metrics = {}
        
        # ë¶„ì„ì— ì¤‘ìš”í•œ ë©”íŠ¸ë¦­ë“¤
        important_metrics = [
            'information_transfer_efficiency', 'learning_convergence_epochs',
            'communication_overhead', 'network_load', 'shannon_entropy',
            'success_rate', 'average_reward'
        ]
        
        for metric_name in important_metrics:
            if metric_name in metrics:
                metric_data = metrics[metric_name]
                if isinstance(metric_data, dict) and 'mean' in metric_data:
                    key_metrics[metric_name] = {
                        'mean': float(metric_data['mean']),
                        'std': float(metric_data.get('std', 0)),
                        'last': float(metric_data.get('last', metric_data['mean']))
                    }
                elif isinstance(metric_data, (int, float)):
                    key_metrics[metric_name] = float(metric_data)
        
        return key_metrics
    
    def analyze_results(self) -> Dict:
        """ê²°ê³¼ ë¶„ì„"""
        logger.info("ê²°ê³¼ ë¶„ì„ ì‹œì‘")
        
        # 1. ì„±ëŠ¥ ë¹„êµ ë¶„ì„
        performance_analysis = self._analyze_performance_differences()
        
        # 2. ì°¨ì›ë³„ ê¸°ì—¬ë„ ë¶„ì„
        contribution_analysis = self._analyze_dimension_contributions()
        
        # 3. í†µê³„ì  ìœ ì˜ì„± ê²€ì •
        statistical_analysis = self._perform_statistical_tests()
        
        # 4. ì°¨ì› ê°„ ì‹œë„ˆì§€ íš¨ê³¼ ë¶„ì„
        synergy_analysis = self._analyze_dimension_synergies()
        
        # 5. ì •ë³´ ì´ë¡ ì  ë¶„ì„
        information_analysis = self._analyze_information_content()
        
        self.analysis_results = {
            'performance_comparison': performance_analysis,
            'dimension_contributions': contribution_analysis,
            'statistical_tests': statistical_analysis,
            'synergy_effects': synergy_analysis,
            'information_analysis': information_analysis,
            'summary': self._generate_analysis_summary()
        }
        
        return self.analysis_results
    
    def _analyze_performance_differences(self) -> Dict:
        """ì„±ëŠ¥ ì°¨ì´ ë¶„ì„"""
        performance_data = {}
        
        # ê° êµ¬ì„±ì—ì„œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì¶œ
        for config_name, results in self.all_results.items():
            successful_results = [r for r in results if r['success']]
            if not successful_results:
                continue
                
            metrics_list = []
            for result in successful_results:
                if 'metrics' in result:
                    key_metrics = self._extract_key_metrics(result['metrics'])
                    metrics_list.append(key_metrics)
            
            if metrics_list:
                # ë©”íŠ¸ë¦­ë³„ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
                config_performance = {}
                for metric_name in self.config['analysis_metrics']['performance_indicators']:
                    values = []
                    for metrics in metrics_list:
                        if metric_name in metrics:
                            if isinstance(metrics[metric_name], dict):
                                values.append(metrics[metric_name].get('mean', 0))
                            else:
                                values.append(metrics[metric_name])
                    
                    if values:
                        config_performance[metric_name] = {
                            'mean': np.mean(values),
                            'std': np.std(values),
                            'min': np.min(values),
                            'max': np.max(values),
                            'count': len(values)
                        }
                
                performance_data[config_name] = config_performance
        
        return performance_data
    
    def _analyze_dimension_contributions(self) -> Dict:
        """ì°¨ì›ë³„ ê¸°ì—¬ë„ ë¶„ì„"""
        contributions = {
            'individual_contributions': {},
            'marginal_contributions': {},
            'interaction_effects': {}
        }
        
        # ê°œë³„ ì°¨ì› ê¸°ì—¬ë„ (ë‹¨ì¼ ì°¨ì› vs ë¬´ì°¨ì›)
        performance_comparison = self.analysis_results.get('performance_comparison', {})
        single_dim_results = {k: v for k, v in performance_comparison.items() 
                             if k.startswith('single_')}
        
        baseline_performance = 0  # ë¬´ì°¨ì› ê¸°ì¤€ (ëª¨ë“  ì°¨ì›ì´ 0ì¸ ê²½ìš°ëŠ” ì‹¤í–‰ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ 0ìœ¼ë¡œ ê°€ì •)
        
        for dim_config, performance in single_dim_results.items():
            dim_name = dim_config.replace('single_', '').replace('_only', '')
            if 'shannon_entropy' in performance:
                contribution = performance['shannon_entropy']['mean'] - baseline_performance
                contributions['individual_contributions'][dim_name] = contribution
        
        # í•œê³„ ê¸°ì—¬ë„ ë¶„ì„ (n-1ì°¨ì›ì—ì„œ nì°¨ì›ìœ¼ë¡œ ì¶”ê°€í•  ë•Œì˜ ê°œì„ ë„)
        full_4d_performance = performance_comparison.get('full_4d', {})
        three_dim_results = {k: v for k, v in performance_comparison.items() 
                           if k.startswith('three_')}
        
        for three_config, three_performance in three_dim_results.items():
            missing_dim = three_config.replace('three_no_', '')
            if 'shannon_entropy' in full_4d_performance and 'shannon_entropy' in three_performance:
                marginal_contribution = (full_4d_performance['shannon_entropy']['mean'] - 
                                       three_performance['shannon_entropy']['mean'])
                contributions['marginal_contributions'][missing_dim] = marginal_contribution
        
        return contributions
    
    def _perform_statistical_tests(self) -> Dict:
        """í†µê³„ì  ìœ ì˜ì„± ê²€ì •"""
        statistical_results = {}
        
        # performance_comparison ë³€ìˆ˜ ì •ì˜
        performance_comparison = self.analysis_results.get('performance_comparison', {})
        
        # ì°¨ì› ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥ ì°¨ì´ (One-way ANOVA)
        dimension_groups = {
            '1D': [k for k in self.all_results.keys() if k.startswith('single_')],
            '2D': [k for k in self.all_results.keys() if k.startswith('two_')],
            '3D': [k for k in self.all_results.keys() if k.startswith('three_')],
            '4D': ['full_4d']
        }
        
        for metric_name in self.config.get('analysis_metrics', {}).get('performance_indicators', ['shannon_entropy']):
            groups_data = []
            group_labels = []
            
            for group_name, config_names in dimension_groups.items():
                group_values = []
                for config_name in config_names:
                    if config_name in performance_comparison:
                        perf_data = performance_comparison[config_name]
                        if metric_name in perf_data:
                            # ëª¨ë“  ì‹¤í–‰ ê²°ê³¼ì—ì„œ ê°’ ì¶”ì¶œ
                            for result in self.all_results[config_name]:
                                if result['success'] and 'metrics' in result:
                                    key_metrics = self._extract_key_metrics(result['metrics'])
                                    if metric_name in key_metrics:
                                        if isinstance(key_metrics[metric_name], dict):
                                            group_values.append(key_metrics[metric_name].get('mean', 0))
                                        else:
                                            group_values.append(key_metrics[metric_name])
                
                if group_values:
                    groups_data.append(group_values)
                    group_labels.append(group_name)
            
            # ANOVA ìˆ˜í–‰
            if len(groups_data) >= 2 and all(len(g) > 1 for g in groups_data):
                f_stat, p_value = f_oneway(*groups_data)
                statistical_results[f'{metric_name}_anova'] = {
                    'f_statistic': float(f_stat),
                    'p_value': float(p_value),
                    'significant': p_value < self.config['statistical_analysis']['significance_level'],
                    'groups': group_labels,
                    'group_sizes': [len(g) for g in groups_data]
                }
        
        return statistical_results
    
    def _analyze_dimension_synergies(self) -> Dict:
        """ì°¨ì› ê°„ ì‹œë„ˆì§€ íš¨ê³¼ ë¶„ì„"""
        synergies = {}
        
        # performance_comparison ë³€ìˆ˜ ì •ì˜
        performance_comparison = self.analysis_results.get('performance_comparison', {})
        
        # 2ì°¨ì› ì¡°í•©ì˜ ì„±ëŠ¥ vs ê°œë³„ ì°¨ì› ì„±ëŠ¥ì˜ í•©
        two_dim_configs = [k for k in self.all_results.keys() if k.startswith('two_')]
        
        for config_name in two_dim_configs:
            if config_name in performance_comparison:
                config_performance = performance_comparison[config_name]
                
                # êµ¬ì„±ì—ì„œ í™œì„± ì°¨ì› ì¶”ì¶œ
                dimension_config = self.dimension_configs[config_name]
                active_dims = [dim for dim, size in dimension_config.items() if size > 0]
                
                if len(active_dims) == 2:
                    # ê°œë³„ ì°¨ì› ì„±ëŠ¥ ê°€ì ¸ì˜¤ê¸°
                    individual_performances = []
                    for dim in active_dims:
                        single_config = f"single_{dim}_only"
                        if single_config in performance_comparison:
                            individual_performances.append(
                                performance_comparison[single_config]
                            )
                    
                    if len(individual_performances) == 2:
                        # Shannon entropyì— ëŒ€í•œ ì‹œë„ˆì§€ ê³„ì‚°
                        if ('shannon_entropy' in config_performance and 
                            all('shannon_entropy' in perf for perf in individual_performances)):
                            
                            combined_performance = config_performance['shannon_entropy']['mean']
                            expected_performance = sum(perf['shannon_entropy']['mean'] 
                                                     for perf in individual_performances)
                            
                            synergy_effect = combined_performance - expected_performance
                            synergies[config_name] = {
                                'dimensions': active_dims,
                                'synergy_effect': synergy_effect,
                                'relative_synergy': synergy_effect / expected_performance if expected_performance != 0 else 0
                            }
        
        return synergies
    
    def _analyze_information_content(self) -> Dict:
        """ì •ë³´ ì´ë¡ ì  ë¶„ì„"""
        information_analysis = {
            'entropy_analysis': {},
            'information_efficiency': {},
            'redundancy_analysis': {}
        }
        
        # ê° ì°¨ì› ì¡°í•©ì˜ ì •ë³´ ì—”íŠ¸ë¡œí”¼ ë¶„ì„
        for config_name, results in self.all_results.items():
            successful_results = [r for r in results if r['success']]
            if not successful_results:
                continue
            
            shannon_entropies = []
            utilization_rates = []
            
            for result in successful_results:
                if 'metrics' in result:
                    key_metrics = self._extract_key_metrics(result['metrics'])
                    if 'shannon_entropy' in key_metrics:
                        if isinstance(key_metrics['shannon_entropy'], dict):
                            shannon_entropies.append(key_metrics['shannon_entropy'].get('mean', 0))
                        else:
                            shannon_entropies.append(key_metrics['shannon_entropy'])
                
                if 'dimension_analysis' in result:
                    dim_analysis = result['dimension_analysis']
                    if 'utilization_rates' in dim_analysis:
                        avg_utilization = np.mean(list(dim_analysis['utilization_rates'].values()))
                        utilization_rates.append(avg_utilization)
            
            if shannon_entropies:
                information_analysis['entropy_analysis'][config_name] = {
                    'mean_entropy': np.mean(shannon_entropies),
                    'entropy_std': np.std(shannon_entropies),
                    'entropy_efficiency': np.mean(shannon_entropies) / len(self.dimension_configs[config_name])  # ì°¨ì› ìˆ˜ë¡œ ì •ê·œí™”
                }
            
            if utilization_rates:
                information_analysis['information_efficiency'][config_name] = {
                    'mean_utilization': np.mean(utilization_rates),
                    'utilization_std': np.std(utilization_rates)
                }
        
        return information_analysis
    
    def _generate_analysis_summary(self) -> Dict:
        """ë¶„ì„ ìš”ì•½ ìƒì„±"""
        # performance_comparison ë³€ìˆ˜ ì •ì˜
        performance_comparison = self.analysis_results.get('performance_comparison', {})
        
        summary = {
            'total_configurations_tested': len(self.dimension_configs),
            'successful_experiments': 0,
            'failed_experiments': 0,
            'best_performing_configuration': None,
            'most_important_dimension': None,
            'key_findings': [],
            'recommendations': []
        }
        
        # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„
        for results in self.all_results.values():
            for result in results:
                if result['success']:
                    summary['successful_experiments'] += 1
                else:
                    summary['failed_experiments'] += 1
        
        # ìµœê³  ì„±ëŠ¥ êµ¬ì„± ì°¾ê¸°
        best_config = None
        best_performance = -float('inf')
        
        for config_name, performance in performance_comparison.items():
            if 'shannon_entropy' in performance:
                entropy = performance['shannon_entropy']['mean']
                if entropy > best_performance:
                    best_performance = entropy
                    best_config = config_name
        
        summary['best_performing_configuration'] = best_config
        
        # ê°€ì¥ ì¤‘ìš”í•œ ì°¨ì› ì°¾ê¸°
        if 'marginal_contributions' in self.analysis_results['dimension_contributions']:
            contributions = self.analysis_results['dimension_contributions']['marginal_contributions']
            if contributions:
                most_important = max(contributions.items(), key=lambda x: x[1])
                summary['most_important_dimension'] = most_important[0]
        
        # ì£¼ìš” ë°œê²¬ì‚¬í•­
        if best_config == 'full_4d':
            summary['key_findings'].append("4D í˜ë¡œëª¬ì´ ìµœê³  ì„±ëŠ¥ì„ ë³´ì„")
        else:
            summary['key_findings'].append(f"ìµœê³  ì„±ëŠ¥: {best_config}")
        
        # í†µê³„ì  ìœ ì˜ì„± í™•ì¸
        significant_tests = [test for test, result in self.analysis_results['statistical_tests'].items() 
                           if result.get('significant', False)]
        if significant_tests:
            summary['key_findings'].append(f"í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ ë°œê²¬: {len(significant_tests)}ê°œ í…ŒìŠ¤íŠ¸")
        
        return summary
    
    def generate_visualizations(self):
        """ë¶„ì„ ê²°ê³¼ ì‹œê°í™”"""
        logger.info("ì‹œê°í™” ìƒì„± ì‹œì‘")
        
        # 1. ì„±ëŠ¥ ë¹„êµ íˆíŠ¸ë§µ
        self._plot_performance_heatmap()
        
        # 2. ì°¨ì›ë³„ ê¸°ì—¬ë„ ë°” ì°¨íŠ¸
        self._plot_dimension_contributions()
        
        # 3. ì‹œë„ˆì§€ íš¨ê³¼ ì‹œê°í™”
        self._plot_synergy_effects()
        
        # 4. í†µê³„ì  ìœ ì˜ì„± í”Œë¡¯
        self._plot_statistical_significance()
        
        # 5. ì •ë³´ íš¨ìœ¨ì„± ë¶„ì„
        self._plot_information_efficiency()
        
        logger.info("ëª¨ë“  ì‹œê°í™” ì™„ë£Œ")
    
    def _plot_performance_heatmap(self):
        """ì„±ëŠ¥ ë¹„êµ íˆíŠ¸ë§µ"""
        performance_data = self.analysis_results.get('performance_comparison', {})
        
        # ë°ì´í„° ì¤€ë¹„
        config_names = list(performance_data.keys())
        metrics = self.config['analysis_metrics']['performance_indicators']
        
        heatmap_data = []
        for metric in metrics:
            row = []
            for config in config_names:
                if config in performance_data and metric in performance_data[config]:
                    value = performance_data[config][metric]['mean']
                else:
                    value = 0
                row.append(value)
            heatmap_data.append(row)
        
        # íˆíŠ¸ë§µ ìƒì„±
        plt.figure(figsize=(15, 8))
        sns.heatmap(heatmap_data, 
                   xticklabels=config_names, 
                   yticklabels=metrics,
                   annot=True, 
                   cmap='viridis', 
                   fmt='.3f')
        plt.title('ì°¨ì› êµ¬ì„±ë³„ ì„±ëŠ¥ ë¹„êµ')
        plt.xlabel('ì°¨ì› êµ¬ì„±')
        plt.ylabel('ì„±ëŠ¥ ì§€í‘œ')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        
        save_path = os.path.join(self.results_dir, 'performance_heatmap.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_dimension_contributions(self):
        """ì°¨ì›ë³„ ê¸°ì—¬ë„ ë°” ì°¨íŠ¸"""
        contributions = self.analysis_results['dimension_contributions']
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # ê°œë³„ ê¸°ì—¬ë„
        if 'individual_contributions' in contributions:
            dims = list(contributions['individual_contributions'].keys())
            values = list(contributions['individual_contributions'].values())
            
            ax1.bar(dims, values, color='skyblue', alpha=0.7)
            ax1.set_title('ê°œë³„ ì°¨ì› ê¸°ì—¬ë„')
            ax1.set_xlabel('ì°¨ì›')
            ax1.set_ylabel('ê¸°ì—¬ë„ (Shannon Entropy)')
            ax1.tick_params(axis='x', rotation=45)
        
        # í•œê³„ ê¸°ì—¬ë„  
        if 'marginal_contributions' in contributions:
            dims = list(contributions['marginal_contributions'].keys())
            values = list(contributions['marginal_contributions'].values())
            
            ax2.bar(dims, values, color='lightcoral', alpha=0.7)
            ax2.set_title('í•œê³„ ê¸°ì—¬ë„ (3D â†’ 4D)')
            ax2.set_xlabel('ì¶”ê°€ëœ ì°¨ì›')
            ax2.set_ylabel('í•œê³„ ê¸°ì—¬ë„')
            ax2.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        save_path = os.path.join(self.results_dir, 'dimension_contributions.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_synergy_effects(self):
        """ì‹œë„ˆì§€ íš¨ê³¼ ì‹œê°í™”"""
        synergies = self.analysis_results['synergy_effects']
        
        if not synergies:
            return
        
        config_names = list(synergies.keys())
        synergy_values = [synergies[config]['synergy_effect'] for config in config_names]
        relative_synergies = [synergies[config]['relative_synergy'] for config in config_names]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # ì ˆëŒ€ ì‹œë„ˆì§€ íš¨ê³¼
        bars1 = ax1.bar(range(len(config_names)), synergy_values, 
                       color=['green' if v > 0 else 'red' for v in synergy_values],
                       alpha=0.7)
        ax1.set_title('ì°¨ì› ê°„ ì‹œë„ˆì§€ íš¨ê³¼ (ì ˆëŒ€ê°’)')
        ax1.set_xlabel('ì°¨ì› ì¡°í•©')
        ax1.set_ylabel('ì‹œë„ˆì§€ íš¨ê³¼')
        ax1.set_xticks(range(len(config_names)))
        ax1.set_xticklabels([name.replace('two_', '') for name in config_names], rotation=45)
        ax1.axhline(y=0, color='black', linestyle='--', alpha=0.5)
        
        # ìƒëŒ€ ì‹œë„ˆì§€ íš¨ê³¼
        bars2 = ax2.bar(range(len(config_names)), relative_synergies, 
                       color=['green' if v > 0 else 'red' for v in relative_synergies],
                       alpha=0.7)
        ax2.set_title('ì°¨ì› ê°„ ì‹œë„ˆì§€ íš¨ê³¼ (ìƒëŒ€ê°’)')
        ax2.set_xlabel('ì°¨ì› ì¡°í•©')
        ax2.set_ylabel('ìƒëŒ€ ì‹œë„ˆì§€ íš¨ê³¼')
        ax2.set_xticks(range(len(config_names)))
        ax2.set_xticklabels([name.replace('two_', '') for name in config_names], rotation=45)
        ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)
        
        plt.tight_layout()
        save_path = os.path.join(self.results_dir, 'synergy_effects.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_statistical_significance(self):
        """í†µê³„ì  ìœ ì˜ì„± í”Œë¡¯"""
        statistical_results = self.analysis_results['statistical_tests']
        
        # ANOVA ê²°ê³¼ë§Œ í•„í„°ë§
        anova_results = {k: v for k, v in statistical_results.items() if k.endswith('_anova')}
        
        if not anova_results:
            return
        
        metrics = [k.replace('_anova', '') for k in anova_results.keys()]
        f_statistics = [anova_results[f'{m}_anova']['f_statistic'] for m in metrics]
        p_values = [anova_results[f'{m}_anova']['p_value'] for m in metrics]
        significance_threshold = self.config['statistical_analysis']['significance_level']
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # F-í†µê³„ëŸ‰
        ax1.bar(range(len(metrics)), f_statistics, color='lightblue', alpha=0.7)
        ax1.set_title('ANOVA F-í†µê³„ëŸ‰')
        ax1.set_xlabel('ì„±ëŠ¥ ì§€í‘œ')
        ax1.set_ylabel('F-í†µê³„ëŸ‰')
        ax1.set_xticks(range(len(metrics)))
        ax1.set_xticklabels(metrics, rotation=45)
        
        # p-ê°’
        colors = ['green' if p < significance_threshold else 'red' for p in p_values]
        ax2.bar(range(len(metrics)), p_values, color=colors, alpha=0.7)
        ax2.axhline(y=significance_threshold, color='black', linestyle='--', 
                   label=f'Î± = {significance_threshold}')
        ax2.set_title('ANOVA p-ê°’')
        ax2.set_xlabel('ì„±ëŠ¥ ì§€í‘œ')
        ax2.set_ylabel('p-ê°’')
        ax2.set_xticks(range(len(metrics)))
        ax2.set_xticklabels(metrics, rotation=45)
        ax2.legend()
        ax2.set_yscale('log')
        
        plt.tight_layout()
        save_path = os.path.join(self.results_dir, 'statistical_significance.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_information_efficiency(self):
        """ì •ë³´ íš¨ìœ¨ì„± ë¶„ì„ í”Œë¡¯"""
        info_analysis = self.analysis_results['information_analysis']
        
        entropy_data = info_analysis.get('entropy_analysis', {})
        efficiency_data = info_analysis.get('information_efficiency', {})
        
        if not entropy_data and not efficiency_data:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # ì—”íŠ¸ë¡œí”¼ íš¨ìœ¨ì„±
        if entropy_data:
            configs = list(entropy_data.keys())
            efficiencies = [entropy_data[config]['entropy_efficiency'] for config in configs]
            
            axes[0, 0].bar(range(len(configs)), efficiencies, color='purple', alpha=0.7)
            axes[0, 0].set_title('ì—”íŠ¸ë¡œí”¼ íš¨ìœ¨ì„± (ì—”íŠ¸ë¡œí”¼/ì°¨ì›ìˆ˜)')
            axes[0, 0].set_xlabel('ì°¨ì› êµ¬ì„±')
            axes[0, 0].set_ylabel('íš¨ìœ¨ì„±')
            axes[0, 0].set_xticks(range(len(configs)))
            axes[0, 0].set_xticklabels([c.replace('_', '\n') for c in configs], rotation=45, ha='right')
        
        # í‰ê·  ì—”íŠ¸ë¡œí”¼
        if entropy_data:
            entropies = [entropy_data[config]['mean_entropy'] for config in configs]
            
            axes[0, 1].bar(range(len(configs)), entropies, color='orange', alpha=0.7)
            axes[0, 1].set_title('í‰ê·  Shannon ì—”íŠ¸ë¡œí”¼')
            axes[0, 1].set_xlabel('ì°¨ì› êµ¬ì„±')
            axes[0, 1].set_ylabel('Shannon ì—”íŠ¸ë¡œí”¼')
            axes[0, 1].set_xticks(range(len(configs)))
            axes[0, 1].set_xticklabels([c.replace('_', '\n') for c in configs], rotation=45, ha='right')
        
        # í™œìš©ë¥ 
        if efficiency_data:
            util_configs = list(efficiency_data.keys())
            utilizations = [efficiency_data[config]['mean_utilization'] for config in util_configs]
            
            axes[1, 0].bar(range(len(util_configs)), utilizations, color='green', alpha=0.7)
            axes[1, 0].set_title('í‰ê·  ì°¨ì› í™œìš©ë¥ ')
            axes[1, 0].set_xlabel('ì°¨ì› êµ¬ì„±')
            axes[1, 0].set_ylabel('í™œìš©ë¥ ')
            axes[1, 0].set_xticks(range(len(util_configs)))
            axes[1, 0].set_xticklabels([c.replace('_', '\n') for c in util_configs], rotation=45, ha='right')
        
        # ì°¨ì› ìˆ˜ vs ì„±ëŠ¥ ì‚°ì ë„
        if entropy_data:
            dimension_counts = []
            mean_entropies = []
            
            for config in configs:
                # ì°¨ì› ìˆ˜ ê³„ì‚°
                dim_config = self.dimension_configs[config]
                active_dims = sum(1 for size in dim_config.values() if size > 0)
                dimension_counts.append(active_dims)
                mean_entropies.append(entropy_data[config]['mean_entropy'])
            
            axes[1, 1].scatter(dimension_counts, mean_entropies, alpha=0.7, s=100)
            axes[1, 1].set_title('ì°¨ì› ìˆ˜ vs ì„±ëŠ¥')
            axes[1, 1].set_xlabel('í™œì„± ì°¨ì› ìˆ˜')
            axes[1, 1].set_ylabel('í‰ê·  Shannon ì—”íŠ¸ë¡œí”¼')
            
            # ì¶”ì„¸ì„  ì¶”ê°€
            z = np.polyfit(dimension_counts, mean_entropies, 1)
            p = np.poly1d(z)
            axes[1, 1].plot(sorted(dimension_counts), p(sorted(dimension_counts)), "r--", alpha=0.7)
        
        plt.tight_layout()
        save_path = os.path.join(self.results_dir, 'information_efficiency.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def save_final_report(self):
        """ìµœì¢… ë³´ê³ ì„œ ì €ì¥"""
        report_path = os.path.join(self.results_dir, 'dimension_ablation_report.md')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# 4D ë””ì§€í„¸ í˜ë¡œëª¬ ì°¨ì›ë³„ ê¸°ì—¬ë„ ë¶„ì„ ë³´ê³ ì„œ\n\n")
            
            # ìš”ì•½ ì •ë³´
            summary = self.analysis_results['summary']
            f.write("## ğŸ”¬ ì‹¤í—˜ ìš”ì•½\n")
            f.write(f"- í…ŒìŠ¤íŠ¸ëœ ì°¨ì› êµ¬ì„±: {summary['total_configurations_tested']}ê°œ\n")
            f.write(f"- ì„±ê³µí•œ ì‹¤í—˜: {summary['successful_experiments']}ê°œ\n")
            f.write(f"- ì‹¤íŒ¨í•œ ì‹¤í—˜: {summary['failed_experiments']}ê°œ\n")
            f.write(f"- ìµœê³  ì„±ëŠ¥ êµ¬ì„±: {summary['best_performing_configuration']}\n")
            f.write(f"- ê°€ì¥ ì¤‘ìš”í•œ ì°¨ì›: {summary['most_important_dimension']}\n\n")
            
            # ì£¼ìš” ë°œê²¬ì‚¬í•­
            f.write("## ğŸ¯ ì£¼ìš” ë°œê²¬ì‚¬í•­\n")
            for finding in summary['key_findings']:
                f.write(f"- {finding}\n")
            f.write("\n")
            
            # ì°¨ì›ë³„ ê¸°ì—¬ë„
            contributions = self.analysis_results['dimension_contributions']
            f.write("## ğŸ“Š ì°¨ì›ë³„ ê¸°ì—¬ë„ ë¶„ì„\n\n")
            
            if 'individual_contributions' in contributions:
                f.write("### ê°œë³„ ì°¨ì› ê¸°ì—¬ë„\n")
                for dim, contrib in contributions['individual_contributions'].items():
                    f.write(f"- **{dim}**: {contrib:.4f}\n")
                f.write("\n")
            
            if 'marginal_contributions' in contributions:
                f.write("### í•œê³„ ê¸°ì—¬ë„ (3D â†’ 4D ì¶”ê°€ ì‹œ)\n")
                for dim, contrib in contributions['marginal_contributions'].items():
                    f.write(f"- **{dim}**: {contrib:.4f}\n")
                f.write("\n")
            
            # í†µê³„ì  ìœ ì˜ì„±
            statistical_tests = self.analysis_results['statistical_tests']
            f.write("## ğŸ“ˆ í†µê³„ì  ìœ ì˜ì„± ê²€ì •\n\n")
            
            significant_count = sum(1 for test in statistical_tests.values() 
                                  if test.get('significant', False))
            f.write(f"í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´: {significant_count}/{len(statistical_tests)}ê°œ í…ŒìŠ¤íŠ¸\n\n")
            
            for test_name, result in statistical_tests.items():
                if result.get('significant', False):
                    f.write(f"- **{test_name}**: F={result['f_statistic']:.3f}, "
                           f"p={result['p_value']:.4f} âœ“\n")
            f.write("\n")
            
            # ì‹œë„ˆì§€ íš¨ê³¼
            synergies = self.analysis_results['synergy_effects']
            if synergies:
                f.write("## ğŸ”„ ì°¨ì› ê°„ ì‹œë„ˆì§€ íš¨ê³¼\n\n")
                for config, synergy_data in synergies.items():
                    dims = " + ".join(synergy_data['dimensions'])
                    effect = synergy_data['synergy_effect']
                    relative = synergy_data['relative_synergy']
                    
                    f.write(f"- **{dims}**: ì‹œë„ˆì§€ íš¨ê³¼ = {effect:.4f} "
                           f"(ìƒëŒ€ê°’: {relative:.2%})\n")
                f.write("\n")
            
            # ê¶Œì¥ì‚¬í•­
            f.write("## ğŸ’¡ ê¶Œì¥ì‚¬í•­\n\n")
            
            if summary['best_performing_configuration'] == 'full_4d':
                f.write("1. **4D í˜ë¡œëª¬ ì‚¬ìš© ê¶Œì¥**: ëª¨ë“  ì°¨ì›ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ìµœì ì˜ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\n")
            else:
                f.write(f"1. **ìµœì  êµ¬ì„± ì ìš©**: {summary['best_performing_configuration']} êµ¬ì„±ì´ "
                       "ìµœê³  ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\n")
            
            if summary['most_important_dimension']:
                f.write(f"2. **í•µì‹¬ ì°¨ì› ì§‘ì¤‘**: {summary['most_important_dimension']} ì°¨ì›ì´ "
                       "ê°€ì¥ í° ê¸°ì—¬ë¥¼ í•©ë‹ˆë‹¤.\n")
            
            f.write("3. **ë¦¬ì†ŒìŠ¤ ìµœì í™”**: ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ê³¼ ê³„ì‚° ë¦¬ì†ŒìŠ¤ë¥¼ ê³ ë ¤í•˜ì—¬ "
                   "ì ì ˆí•œ ì°¨ì› ì¡°í•©ì„ ì„ íƒí•˜ì„¸ìš”.\n")
            
            # ì‹¤í—˜ ì„¸ë¶€ì‚¬í•­
            f.write("\n## ğŸ“‹ ì‹¤í—˜ ì„¸ë¶€ì‚¬í•­\n\n")
            f.write(f"- ì‹¤í–‰ í™˜ê²½: {self.config['experiment']['name']}\n")
            f.write(f"- ë°˜ë³µ ì‹¤í–‰ íšŸìˆ˜: {self.config['execution']['runs_per_configuration']}íšŒ\n")
            f.write(f"- ìœ ì˜ìˆ˜ì¤€: {self.config['statistical_analysis']['significance_level']}\n")
            f.write(f"- ë¶„ì„ ì¼ì‹œ: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            
        logger.info(f"ìµœì¢… ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {report_path}")
    
    def run_complete_analysis(self):
        """ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("4D í˜ë¡œëª¬ ì°¨ì›ë³„ ê¸°ì—¬ë„ ë¶„ì„ ì‹œì‘")
        
        try:
            # 1. ëª¨ë“  ì‹¤í—˜ ì‹¤í–‰
            self.run_all_experiments()
            
            # 2. ê²°ê³¼ ë¶„ì„
            self.analyze_results()
            
            # 3. ì‹œê°í™” ìƒì„±
            self.generate_visualizations()
            
            # 4. ìµœì¢… ë³´ê³ ì„œ ì €ì¥
            self.save_final_report()
            
            logger.info("ì°¨ì›ë³„ ê¸°ì—¬ë„ ë¶„ì„ ì™„ë£Œ")
            
            return self.analysis_results
            
        except Exception as e:
            logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
        finally:
            if ray.is_initialized():
                ray.shutdown()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='4D í˜ë¡œëª¬ ì°¨ì›ë³„ ê¸°ì—¬ë„ ë¶„ì„')
    parser.add_argument('--config', type=str, 
                       default='config/ablation_dimension_config.yaml',
                       help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    # ë¶„ì„ ì‹¤í–‰
    ablation_study = DimensionAblationStudy(args.config)
    results = ablation_study.run_complete_analysis()
    
    print("\n" + "="*60)
    print("4D ë””ì§€í„¸ í˜ë¡œëª¬ ì°¨ì›ë³„ ê¸°ì—¬ë„ ë¶„ì„ ì™„ë£Œ")
    print("="*60)
    print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {ablation_study.results_dir}")
    print(f"ìµœê³  ì„±ëŠ¥ êµ¬ì„±: {results['summary']['best_performing_configuration']}")
    print(f"ê°€ì¥ ì¤‘ìš”í•œ ì°¨ì›: {results['summary']['most_important_dimension']}")
    print("="*60)


if __name__ == "__main__":
    main()