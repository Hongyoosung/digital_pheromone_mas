import os
import yaml
import torch
import numpy as np
import ray
from typing import Dict, List
import time
import logging
from tqdm import tqdm
import argparse
import pickle
import json
import pandas as pd
from scipy import stats

from src.experiments.run_experiment import ExperimentRunner
from src.utils.metrics import MetricsTracker
from src.utils.visualization import ExperimentVisualizer

logger = logging.getLogger(__name__)

class ComparisonExperimentRunner:
    """ì—°êµ¬ ê³„íšì„œ ëª…ì‹œ ë¹„êµ ì‹¤í—˜ ì‹¤í–‰ê¸°"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.results_dir = "results/comparison/"
        os.makedirs(self.results_dir, exist_ok=True)
        
        # ë¹„êµ ëŒ€ìƒ ì„¤ì •
        self.baseline_methods = self.config['research_design']['baseline_methods']
        self.num_runs = self.config['research_design']['num_runs']
        self.significance_level = self.config['research_design']['statistical_analysis']['significance_level']
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.comparison_results = {}
        
    def run_baseline_experiment(self, method: str, run_id: int) -> Dict:
        """ê¸°ì¤€ì„  ì‹¤í—˜ ì‹¤í–‰"""
        logger.info(f"ê¸°ì¤€ì„  ì‹¤í—˜ ì‹¤í–‰: {method}, Run {run_id}")
        
        # ê¸°ì¤€ì„ ë³„ ì„¤ì • ìˆ˜ì •
        baseline_config = self.config.copy()
        
        if method == "rule_based_diffusion":
            # ê·œì¹™ ê¸°ë°˜ í™•ì‚° ëª¨ë¸
            baseline_config['pheromone']['decay_rate'] = 0.98  # ë‹¨ìˆœ ê°ì‡ 
            baseline_config['attention']['num_heads'] = 1  # ë‹¨ìˆœ ì–´í…ì…˜
            baseline_config['hyperparameters']['communication_period'] = [10]  # ë‚®ì€ í†µì‹  ë¹ˆë„
            
        elif method == "centralized_attention":
            # ì¤‘ì•™ì§‘ì¤‘ ì–´í…ì…˜ ë„¤íŠ¸ì›Œí¬
            baseline_config['attention']['topology_type'] = "centralized"
            baseline_config['hyperparameters']['communication_period'] = [1]  # ë†’ì€ í†µì‹  ë¹ˆë„
            
        # ê¸°ì¤€ì„  ì‹¤í—˜ ì‹¤í–‰
        runner = ExperimentRunner(config_path=None)  # ì„¤ì •ì„ ì§ì ‘ ì „ë‹¬
        runner.config = baseline_config
        runner.setup_experiment()
        
        results = runner.run_experiment()
        
        # ê²°ê³¼ì— ë©”ì„œë“œ ì •ë³´ ì¶”ê°€
        results['method'] = method
        results['run_id'] = run_id
        
        return results
    
    def run_proposed_method(self, run_id: int) -> Dict:
        """ì œì•ˆ ë°©ë²• ì‹¤í—˜ ì‹¤í–‰"""
        logger.info(f"ì œì•ˆ ë°©ë²• ì‹¤í—˜ ì‹¤í–‰: Run {run_id}")
        
        runner = ExperimentRunner(config_path=None)
        runner.config = self.config
        runner.setup_experiment()
        
        results = runner.run_experiment()
        results['method'] = 'proposed_digital_pheromone'
        results['run_id'] = run_id
        
        return results
    
    def run_comparison_experiment(self):
        """ì „ì²´ ë¹„êµ ì‹¤í—˜ ì‹¤í–‰"""
        logger.info("ì—°êµ¬ ê³„íšì„œ ëª…ì‹œ ë¹„êµ ì‹¤í—˜ ì‹œì‘")
        
        all_results = []
        
        # ì œì•ˆ ë°©ë²• ì‹¤í—˜ (10íšŒ ë°˜ë³µ)
        logger.info("ì œì•ˆ ë°©ë²• ì‹¤í—˜ ì‹¤í–‰ ì¤‘...")
        for run_id in tqdm(range(self.num_runs), desc="ì œì•ˆ ë°©ë²•"):
            try:
                results = self.run_proposed_method(run_id)
                all_results.append(results)
                
                # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
                with open(os.path.join(self.results_dir, f"proposed_run_{run_id}.pkl"), 'wb') as f:
                    pickle.dump(results, f)
                    
            except Exception as e:
                logger.error(f"ì œì•ˆ ë°©ë²• Run {run_id} ì‹¤íŒ¨: {e}")
        
        # ê¸°ì¤€ì„  ë°©ë²•ë“¤ ì‹¤í—˜
        for method in self.baseline_methods:
            logger.info(f"{method} ê¸°ì¤€ì„  ì‹¤í—˜ ì‹¤í–‰ ì¤‘...")
            for run_id in tqdm(range(self.num_runs), desc=method):
                try:
                    results = self.run_baseline_experiment(method, run_id)
                    all_results.append(results)
                    
                    # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
                    with open(os.path.join(self.results_dir, f"{method}_run_{run_id}.pkl"), 'wb') as f:
                        pickle.dump(results, f)
                        
                except Exception as e:
                    logger.error(f"{method} Run {run_id} ì‹¤íŒ¨: {e}")
        
        # ê²°ê³¼ ë¶„ì„
        self.analyze_comparison_results(all_results)
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        with open(os.path.join(self.results_dir, "comparison_results.pkl"), 'wb') as f:
            pickle.dump(all_results, f)
        
        logger.info("ë¹„êµ ì‹¤í—˜ ì™„ë£Œ")
        return all_results
    
    def analyze_comparison_results(self, all_results: List[Dict]):
        """ë¹„êµ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„"""
        logger.info("ë¹„êµ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ì¤‘...")
        
        # ê²°ê³¼ë¥¼ ë©”ì„œë“œë³„ë¡œ ë¶„ë¥˜
        method_results = {}
        for result in all_results:
            method = result['method']
            if method not in method_results:
                method_results[method] = []
            method_results[method].append(result)
        
        # ì£¼ìš” ì§€í‘œë“¤ ì¶”ì¶œ
        metrics_to_analyze = [
            'information_transfer_efficiency',
            'learning_convergence_epochs', 
            'communication_overhead',
            'network_load',
            'shannon_entropy',
            'success_rate',
            'reward'
        ]
        
        analysis_results = {}
        
        for metric in metrics_to_analyze:
            metric_data = {}
            
            for method, results in method_results.items():
                values = []
                for result in results:
                    # ë©”íŠ¸ë¦­ ê°’ ì¶”ì¶œ
                    if metric in result.get('training_summary', {}).get('research_metrics', {}):
                        value = result['training_summary']['research_metrics'][metric]
                    elif metric in result.get('training_summary', {}).get('performance_analysis', {}):
                        value = result['training_summary']['performance_analysis'][metric].get('final', 0)
                    else:
                        value = 0.0
                    
                    values.append(value)
                
                metric_data[method] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values),
                    'values': values
                }
            
            analysis_results[metric] = metric_data
        
        # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
        statistical_tests = {}
        
        for metric, metric_data in analysis_results.items():
            if 'proposed_digital_pheromone' in metric_data:
                proposed_values = metric_data['proposed_digital_pheromone']['values']
                
                for method in self.baseline_methods:
                    if method in metric_data:
                        baseline_values = metric_data[method]['values']
                        
                        # t-ê²€ì • ìˆ˜í–‰
                        t_stat, p_value = stats.ttest_ind(proposed_values, baseline_values)
                        
                        statistical_tests[f"{metric}_{method}"] = {
                            't_statistic': t_stat,
                            'p_value': p_value,
                            'significant': p_value < self.significance_level,
                            'effect_size': (np.mean(proposed_values) - np.mean(baseline_values)) / np.sqrt(
                                (np.var(proposed_values) + np.var(baseline_values)) / 2
                            )
                        }
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.comparison_results = {
            'method_results': method_results,
            'analysis_results': analysis_results,
            'statistical_tests': statistical_tests
        }
        
        # ë¶„ì„ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
        self.save_analysis_to_csv(analysis_results, statistical_tests)
        
        # ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
        self.generate_comparison_report(analysis_results, statistical_tests)
    
    def save_analysis_to_csv(self, analysis_results: Dict, statistical_tests: Dict):
        """ë¶„ì„ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        # ë©”íŠ¸ë¦­ë³„ ê²°ê³¼ í…Œì´ë¸”
        metric_data = []
        for metric, metric_data_dict in analysis_results.items():
            for method, stats_dict in metric_data_dict.items():
                metric_data.append({
                    'metric': metric,
                    'method': method,
                    'mean': stats_dict['mean'],
                    'std': stats_dict['std'],
                    'min': stats_dict['min'],
                    'max': stats_dict['max']
                })
        
        df_metrics = pd.DataFrame(metric_data)
        df_metrics.to_csv(os.path.join(self.results_dir, "comparison_metrics.csv"), index=False)
        
        # í†µê³„ ê²€ì • ê²°ê³¼ í…Œì´ë¸”
        test_data = []
        for test_name, test_result in statistical_tests.items():
            test_data.append({
                'test': test_name,
                't_statistic': test_result['t_statistic'],
                'p_value': test_result['p_value'],
                'significant': test_result['significant'],
                'effect_size': test_result['effect_size']
            })
        
        df_tests = pd.DataFrame(test_data)
        df_tests.to_csv(os.path.join(self.results_dir, "statistical_tests.csv"), index=False)
    
    def generate_comparison_report(self, analysis_results: Dict, statistical_tests: Dict):
        """ë¹„êµ ì‹¤í—˜ ë³´ê³ ì„œ ìƒì„±"""
        report_path = os.path.join(self.results_dir, "comparison_report.txt")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("4D ë””ì§€í„¸ í˜ë¡œëª¬ MAS ë¹„êµ ì‹¤í—˜ ë³´ê³ ì„œ\n")
            f.write("=" * 80 + "\n\n")
            
            f.write("ğŸ“Š ì‹¤í—˜ ê°œìš”\n")
            f.write("-" * 40 + "\n")
            f.write(f"ì œì•ˆ ë°©ë²•: 4D ë””ì§€í„¸ í˜ë¡œëª¬ + ë¶„ì‚° ì–´í…ì…˜ ë„¤íŠ¸ì›Œí¬\n")
            f.write(f"ê¸°ì¤€ì„  ë°©ë²•: {', '.join(self.baseline_methods)}\n")
            f.write(f"ë°˜ë³µ ì‹¤í–‰ íšŸìˆ˜: {self.num_runs}íšŒ\n")
            f.write(f"ìœ ì˜ìˆ˜ì¤€: Î± = {self.significance_level}\n\n")
            
            f.write("ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼\n")
            f.write("-" * 40 + "\n")
            
            for metric, metric_data in analysis_results.items():
                f.write(f"\n{metric.upper()}:\n")
                for method, stats in metric_data.items():
                    f.write(f"  {method}: {stats['mean']:.4f} Â± {stats['std']:.4f} "
                           f"(ë²”ìœ„: {stats['min']:.4f} - {stats['max']:.4f})\n")
            
            f.write("\nğŸ”¬ í†µê³„ì  ìœ ì˜ì„± ê²€ì • ê²°ê³¼\n")
            f.write("-" * 40 + "\n")
            
            significant_tests = [name for name, result in statistical_tests.items() 
                               if result['significant']]
            
            if significant_tests:
                f.write("í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ë¥¼ ë³´ì¸ ì§€í‘œë“¤:\n")
                for test_name in significant_tests:
                    result = statistical_tests[test_name]
                    f.write(f"  - {test_name}: t = {result['t_statistic']:.3f}, "
                           f"p = {result['p_value']:.4f}, "
                           f"íš¨ê³¼í¬ê¸° = {result['effect_size']:.3f}\n")
            else:
                f.write("í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ë¥¼ ë³´ì¸ ì§€í‘œê°€ ì—†ìŠµë‹ˆë‹¤.\n")
            
            f.write("\nğŸ’¡ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­\n")
            f.write("-" * 40 + "\n")
            
            # ì„±ëŠ¥ í–¥ìƒë„ ê³„ì‚°
            improvements = {}
            for metric, metric_data in analysis_results.items():
                if 'proposed_digital_pheromone' in metric_data:
                    proposed_mean = metric_data['proposed_digital_pheromone']['mean']
                    
                    for method in self.baseline_methods:
                        if method in metric_data:
                            baseline_mean = metric_data[method]['mean']
                            improvement = ((proposed_mean - baseline_mean) / baseline_mean) * 100
                            improvements[f"{metric}_{method}"] = improvement
            
            if improvements:
                f.write("ì œì•ˆ ë°©ë²•ì˜ ì„±ëŠ¥ í–¥ìƒë„:\n")
                for key, improvement in improvements.items():
                    f.write(f"  - {key}: {improvement:+.2f}%\n")
            
            f.write("\n" + "=" * 80 + "\n")
            f.write("ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ\n")
            f.write("=" * 80 + "\n")
        
        logger.info(f"ë¹„êµ ì‹¤í—˜ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_path}")

def main():
    parser = argparse.ArgumentParser(description="Run Digital Pheromone MAS Comparison Experiment")
    parser.add_argument('--config', type=str, default='config/experiment_config.yaml',
                        help='Path to the experiment configuration file.')
    args = parser.parse_args()
    
    # ë¹„êµ ì‹¤í—˜ ì‹¤í–‰
    comparison_runner = ComparisonExperimentRunner(config_path=args.config)
    results = comparison_runner.run_comparison_experiment()
    
    logger.info("ë¹„êµ ì‹¤í—˜ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
