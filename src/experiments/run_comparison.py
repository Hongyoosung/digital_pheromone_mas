import os
import yaml
import torch
import numpy as np
import ray
from typing import Dict, List
import time
import logging
from tqdm import tqdm
import argparse
import pickle
import json
import pandas as pd
from scipy import stats

from src.experiments.run_experiment import ExperimentRunner
from src.utils.metrics import MetricsTracker
from src.utils.visualization import ExperimentVisualizer
from src.models.baseline_models import BaselineComparator

logger = logging.getLogger(__name__)

class ComparisonExperimentRunner:
    """ì—°êµ¬ ê³„íšì„œ ëª…ì‹œ ë¹„êµ ì‹¤í—˜ ì‹¤í–‰ê¸°"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # ë¹„êµêµ° ì‹¤í—˜ ì „ìš© ê²°ê³¼ ë””ë ‰í† ë¦¬
        self.results_dir = "results/comparison/"
        self.proposed_dir = os.path.join(self.results_dir, "proposed_method")
        self.baseline_dir = os.path.join(self.results_dir, "baseline_methods")
        
        # ê° ë°©ë²•ë¡ ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.proposed_dir, exist_ok=True)
        os.makedirs(self.baseline_dir, exist_ok=True)
        
        for method in self.config['research_design']['baseline_methods']:
            method_dir = os.path.join(self.baseline_dir, method)
            os.makedirs(method_dir, exist_ok=True)
        
        logger.info(f"ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ: {self.results_dir}")
        logger.info(f"ì œì•ˆ ë°©ë²• ë””ë ‰í† ë¦¬: {self.proposed_dir}")
        logger.info(f"ê¸°ì¤€ì„  ë°©ë²• ë””ë ‰í† ë¦¬: {self.baseline_dir}")
        
        # ë¹„êµ ëŒ€ìƒ ì„¤ì •
        self.baseline_methods = self.config['research_design']['baseline_methods']
        self.num_runs = self.config['research_design']['num_runs']
        self.significance_level = self.config['research_design']['statistical_analysis']['significance_level']
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.comparison_results = {}
        
        # ë¹„êµêµ° ëª¨ë¸ ì´ˆê¸°í™”
        self.baseline_comparator = BaselineComparator(self.config)
        
    def run_baseline_experiment(self, method: str, run_id: int) -> Dict:
        """ê¸°ì¤€ì„  ì‹¤í—˜ ì‹¤í–‰"""
        logger.info(f"ê¸°ì¤€ì„  ì‹¤í—˜ ì‹¤í–‰: {method}, Run {run_id}")
        
        # ê¸°ì¤€ì„ ë³„ ì„¤ì • ìˆ˜ì •
        baseline_config = self.config.copy()
        
        if method == "rule_based_diffusion":
            # ê·œì¹™ ê¸°ë°˜ í™•ì‚° ëª¨ë¸ ì„¤ì •
            baseline_config['models']['use_rule_based'] = True
            baseline_config['pheromone']['decay_rate'] = 0.1
            baseline_config['attention']['num_heads'] = 1
            baseline_config['hyperparameters']['communication_period'] = [10]
            
        elif method == "centralized_attention":
            # ì¤‘ì•™ì§‘ì¤‘ ì–´í…ì…˜ ë„¤íŠ¸ì›Œí¬ ì„¤ì •
            baseline_config['models']['use_centralized'] = True
            baseline_config['attention']['topology_type'] = "centralized"
            baseline_config['hyperparameters']['communication_period'] = [1]
            
        elif method == "ablation_2d_pheromone":
            # 2D í˜ë¡œëª¬ ì‹¤í—˜ ì„¤ì • - ì‚¬íšŒê´€ê³„ì™€ í™˜ê²½ë§¥ë½ ì œì™¸
            baseline_config['models']['use_2d_pheromone'] = True
            baseline_config['pheromone']['dimensions'] = {
                'behavior': 4,  # í–‰ë™ ì°¨ì›
                'emotion': 5,   # ê°ì • ì°¨ì›
                'social': 0,    # ì‚¬íšŒê´€ê³„ ì°¨ì› ì œì™¸ (0ìœ¼ë¡œ ì„¤ì •)
                'context': 0    # í™˜ê²½ë§¥ë½ ì°¨ì› ì œì™¸ (0ìœ¼ë¡œ ì„¤ì •)
            }
            
        # ì„ì‹œ ì„¤ì • íŒŒì¼ ìƒì„±
        temp_config_path = f"temp_config_{method}_{run_id}.yaml"
        with open(temp_config_path, 'w', encoding='utf-8') as f:
            yaml.dump(baseline_config, f, allow_unicode=True)
        
        # ê¸°ì¤€ì„  ì‹¤í—˜ ì‹¤í–‰
        runner = ExperimentRunner(config_path=temp_config_path)
        
        # ë¹„êµêµ° ëª¨ë¸ ì ìš©
        if hasattr(runner, 'trainer') and runner.trainer:
            runner.trainer.baseline_comparator = self.baseline_comparator
        
        try:
            results = runner.run_experiment()
            
            # ë¹„êµêµ° ì‹¤í—˜ ê²°ê³¼ ì¶”ê°€
            if hasattr(runner, 'attention_router') and runner.attention_router:
                embed_dim = baseline_config.get('embed_dim', 64)
                agent_embeddings = torch.randn(1, 5, embed_dim)  # ì„¤ì •ì— ë§ëŠ” ì°¨ì› ì‚¬ìš©
                pheromone_field = torch.randn(1, 4, 25, 25)  # ë¹ ë¥¸ ì‹¤í—˜ ë§µ í¬ê¸°ì— ë§ì¶¤
                
                comparison_results = self.baseline_comparator.run_comparison_experiment(
                    agent_embeddings, pheromone_field, timestep=50
                )
                
                comparison_metrics = self.baseline_comparator.get_comparison_metrics(comparison_results)
                results['baseline_comparison'] = {
                    'results': comparison_results,
                    'metrics': comparison_metrics
                }
            
            # ê²°ê³¼ì— ë©”ì„œë“œ ì •ë³´ ì¶”ê°€
            results['method'] = method
            results['run_id'] = run_id
            
            return results
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_config_path):
                os.remove(temp_config_path)
    
    def run_proposed_method(self, run_id: int) -> Dict:
        """ì œì•ˆ ë°©ë²• ì‹¤í—˜ ì‹¤í–‰"""
        logger.info(f"ì œì•ˆ ë°©ë²• ì‹¤í—˜ ì‹¤í–‰: Run {run_id}")
        
        # ì„ì‹œ ì„¤ì • íŒŒì¼ ìƒì„±
        temp_config_path = f"temp_config_proposed_{run_id}.yaml"
        with open(temp_config_path, 'w', encoding='utf-8') as f:
            yaml.dump(self.config, f, allow_unicode=True)
        
        try:
            runner = ExperimentRunner(config_path=temp_config_path)
            results = runner.run_experiment()
            results['method'] = 'proposed_digital_pheromone'
            results['run_id'] = run_id
            return results
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_config_path):
                os.remove(temp_config_path)
    
    def run_comparison_experiment(self):
        """ì „ì²´ ë¹„êµ ì‹¤í—˜ ì‹¤í–‰"""
        logger.info("ì—°êµ¬ ê³„íšì„œ ëª…ì‹œ ë¹„êµ ì‹¤í—˜ ì‹œì‘")
        
        all_results = []
        
        # ì œì•ˆ ë°©ë²• ì‹¤í—˜ (10íšŒ ë°˜ë³µ)
        logger.info("ì œì•ˆ ë°©ë²• ì‹¤í—˜ ì‹¤í–‰ ì¤‘...")
        for run_id in tqdm(range(self.num_runs), desc="ì œì•ˆ ë°©ë²•"):
            try:
                results = self.run_proposed_method(run_id)
                all_results.append(results)
                
                # ì œì•ˆ ë°©ë²• ê²°ê³¼ë¥¼ ì „ìš© ë””ë ‰í† ë¦¬ì— ì €ì¥ (pklê³¼ json ëª¨ë‘)
                pkl_path = os.path.normpath(os.path.join(self.proposed_dir, f"proposed_run_{run_id}.pkl"))
                with open(pkl_path, 'wb') as f:
                    pickle.dump(results, f)
                
                # JSON í˜•íƒœë¡œë„ ì €ì¥ (ê°€ë…ì„±ì„ ìœ„í•´)
                json_path = os.path.normpath(os.path.join(self.proposed_dir, f"proposed_run_{run_id}.json"))
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2, default=str)
                
                logger.info(f"ì œì•ˆ ë°©ë²• Run {run_id} ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {pkl_path}")
                    
            except Exception as e:
                logger.error(f"ì œì•ˆ ë°©ë²• Run {run_id} ì‹¤íŒ¨: {e}")
        
        # ê¸°ì¤€ì„  ë°©ë²•ë“¤ ì‹¤í—˜
        for method in self.baseline_methods:
            logger.info(f"{method} ê¸°ì¤€ì„  ì‹¤í—˜ ì‹¤í–‰ ì¤‘...")
            for run_id in tqdm(range(self.num_runs), desc=method):
                try:
                    results = self.run_baseline_experiment(method, run_id)
                    all_results.append(results)
                    
                    # ê¸°ì¤€ì„  ë°©ë²•ë³„ ì „ìš© ë””ë ‰í† ë¦¬ì— ê²°ê³¼ ì €ì¥ (pklê³¼ json ëª¨ë‘)
                    method_dir = os.path.join(self.baseline_dir, method)
                    pkl_path = os.path.normpath(os.path.join(method_dir, f"{method}_run_{run_id}.pkl"))
                    with open(pkl_path, 'wb') as f:
                        pickle.dump(results, f)
                    
                    # JSON í˜•íƒœë¡œë„ ì €ì¥
                    json_path = os.path.normpath(os.path.join(method_dir, f"{method}_run_{run_id}.json"))
                    with open(json_path, 'w', encoding='utf-8') as f:
                        json.dump(results, f, ensure_ascii=False, indent=2, default=str)
                    
                    logger.info(f"ê¸°ì¤€ì„  ë°©ë²• {method} Run {run_id} ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {pkl_path}")
                        
                except Exception as e:
                    logger.error(f"{method} Run {run_id} ì‹¤íŒ¨: {e}")
        
        # ê²°ê³¼ ë¶„ì„
        self.analyze_comparison_results(all_results)
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        final_results_path = os.path.normpath(os.path.join(self.results_dir, "comparison_results.pkl"))
        with open(final_results_path, 'wb') as f:
            pickle.dump(all_results, f)
        
        logger.info(f"ìµœì¢… ë¹„êµ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {final_results_path}")
        
        logger.info("ë¹„êµ ì‹¤í—˜ ì™„ë£Œ")
        return all_results
    
    def analyze_comparison_results(self, all_results: List[Dict]):
        """ë¹„êµ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„"""
        logger.info("ë¹„êµ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ì¤‘...")
        
        # ê²°ê³¼ë¥¼ ë©”ì„œë“œë³„ë¡œ ë¶„ë¥˜
        method_results = {}
        for result in all_results:
            method = result['method']
            if method not in method_results:
                method_results[method] = []
            method_results[method].append(result)
        
        # ì£¼ìš” ì§€í‘œë“¤ ì¶”ì¶œ
        metrics_to_analyze = [
            'information_transfer_efficiency',
            'learning_convergence_epochs', 
            'communication_overhead',
            'network_load',
            'ray_communication_overhead',
            'ray_network_load', 
            'ray_bandwidth_utilization',
            'shannon_entropy',
            'success_rate',
            'reward'
        ]
        
        analysis_results = {}
        
        for metric in metrics_to_analyze:
            metric_data = {}
            
            for method, results in method_results.items():
                values = []
                for result in results:
                    # ë©”íŠ¸ë¦­ ê°’ ì¶”ì¶œ - ë‹¤ì–‘í•œ ê²½ë¡œì—ì„œ ë©”íŠ¸ë¦­ ê²€ìƒ‰
                    value = 0.0
                    
                    # 1. summary.research_metricsì—ì„œ ê²€ìƒ‰
                    if 'summary' in result and 'research_metrics' in result['summary']:
                        if metric in result['summary']['research_metrics']:
                            value = result['summary']['research_metrics'][metric]
                    # 2. training_summary ê²½ë¡œì—ì„œ ê²€ìƒ‰ (ê¸°ì¡´)
                    elif metric in result.get('training_summary', {}).get('research_metrics', {}):
                        value = result['training_summary']['research_metrics'][metric]
                    elif metric in result.get('training_summary', {}).get('performance_analysis', {}):
                        perf_metric = result['training_summary']['performance_analysis'][metric]
                        if isinstance(perf_metric, dict):
                            value = perf_metric.get('final', 0)
                        else:
                            value = perf_metric if perf_metric is not None else 0.0
                    # 3. ì§ì ‘ ê²°ê³¼ì—ì„œ ê²€ìƒ‰
                    elif metric in result:
                        value = result[metric]
                    # 4. metrics í‚¤ ì•„ë˜ì—ì„œ ê²€ìƒ‰
                    elif 'metrics' in result and isinstance(result['metrics'], list) and result['metrics']:
                        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ë©”íŠ¸ë¦­ ì‚¬ìš©
                        last_metrics = result['metrics'][-1] if result['metrics'] else {}
                        if metric in last_metrics:
                            value = last_metrics[metric]
                    
                    # ê°’ì´ ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° 0.0ìœ¼ë¡œ ì„¤ì •
                    if not isinstance(value, (int, float, np.generic)):
                        value = 0.0

                    values.append(float(value))
                
                metric_data[method] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values),
                    'values': values
                }
            
            analysis_results[metric] = metric_data
        
        # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
        statistical_tests = {}
        
        for metric, metric_data in analysis_results.items():
            if 'proposed_digital_pheromone' in metric_data:
                proposed_values = metric_data['proposed_digital_pheromone']['values']
                
                for method in self.baseline_methods:
                    if method in metric_data:
                        baseline_values = metric_data[method]['values']
                        
                        # t-ê²€ì • ìˆ˜í–‰
                        t_stat, p_value = stats.ttest_ind(proposed_values, baseline_values)
                        
                        statistical_tests[f"{metric}_{method}"] = {
                            't_statistic': t_stat,
                            'p_value': p_value,
                            'significant': p_value < self.significance_level,
                            'effect_size': (np.mean(proposed_values) - np.mean(baseline_values)) / np.sqrt(
                                (np.var(proposed_values) + np.var(baseline_values)) / 2
                            )
                        }
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.comparison_results = {
            'method_results': method_results,
            'analysis_results': analysis_results,
            'statistical_tests': statistical_tests
        }
        
        # ë¶„ì„ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥ (ë¹„êµêµ° ê²°ê³¼ ë””ë ‰í† ë¦¬ì—)
        self.save_analysis_to_csv(analysis_results, statistical_tests)
        
        # ì‹¤í—˜ ìš”ì•½ í†µê³„ ì €ì¥
        self.save_experiment_summary(analysis_results, statistical_tests)
        
        # ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
        self.generate_comparison_report(analysis_results, statistical_tests)
        
        # JSON í˜•íƒœë¡œë„ ê²°ê³¼ ì €ì¥
        self.save_results_as_json(analysis_results, statistical_tests)
        
        # ë¹„êµ ì‹¤í—˜ìš© training_summary.txt ìƒì„±
        self.save_comparison_training_summary_to_file(all_results, analysis_results, statistical_tests)
        
        logger.info(f"ë¹„êµ ì‹¤í—˜ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {self.results_dir}")
    
    def save_analysis_to_csv(self, analysis_results: Dict, statistical_tests: Dict):
        """ë¶„ì„ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        # ë©”íŠ¸ë¦­ë³„ ê²°ê³¼ í…Œì´ë¸”
        metric_data = []
        for metric, metric_data_dict in analysis_results.items():
            for method, stats_dict in metric_data_dict.items():
                metric_data.append({
                    'metric': metric,
                    'method': method,
                    'mean': stats_dict['mean'],
                    'std': stats_dict['std'],
                    'min': stats_dict['min'],
                    'max': stats_dict['max']
                })
        
        df_metrics = pd.DataFrame(metric_data)
        df_metrics.to_csv(os.path.join(self.results_dir, "comparison_metrics.csv"), index=False)
        
        # í†µê³„ ê²€ì • ê²°ê³¼ í…Œì´ë¸”
        test_data = []
        for test_name, test_result in statistical_tests.items():
            test_data.append({
                'test': test_name,
                't_statistic': test_result['t_statistic'],
                'p_value': test_result['p_value'],
                'significant': test_result['significant'],
                'effect_size': test_result['effect_size']
            })
        
        df_tests = pd.DataFrame(test_data)
        df_tests.to_csv(os.path.join(self.results_dir, "statistical_tests.csv"), index=False)
    
    def generate_comparison_report(self, analysis_results: Dict, statistical_tests: Dict):
        """ë¹„êµ ì‹¤í—˜ ë³´ê³ ì„œ ìƒì„±"""
        report_path = os.path.join(self.results_dir, "comparison_report.txt")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("4D ë””ì§€í„¸ í˜ë¡œëª¬ MAS ë¹„êµ ì‹¤í—˜ ë³´ê³ ì„œ\n")
            f.write("=" * 80 + "\n\n")
            
            f.write("ğŸ“Š ì‹¤í—˜ ê°œìš”\n")
            f.write("-" * 40 + "\n")
            f.write(f"ì œì•ˆ ë°©ë²•: 4D ë””ì§€í„¸ í˜ë¡œëª¬ + ë¶„ì‚° ì–´í…ì…˜ ë„¤íŠ¸ì›Œí¬\n")
            f.write(f"ê¸°ì¤€ì„  ë°©ë²•: {', '.join(self.baseline_methods)}\n")
            f.write(f"ë°˜ë³µ ì‹¤í–‰ íšŸìˆ˜: {self.num_runs}íšŒ\n")
            f.write(f"ìœ ì˜ìˆ˜ì¤€: Î± = {self.significance_level}\n\n")
            
            f.write("ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼\n")
            f.write("-" * 40 + "\n")
            
            for metric, metric_data in analysis_results.items():
                f.write(f"\n{metric.upper()}:\n")
                for method, stats in metric_data.items():
                    f.write(f"  {method}: {stats['mean']:.4f} Â± {stats['std']:.4f} "
                           f"(ë²”ìœ„: {stats['min']:.4f} - {stats['max']:.4f})\n")
            
            f.write("\nğŸ”¬ í†µê³„ì  ìœ ì˜ì„± ê²€ì • ê²°ê³¼\n")
            f.write("-" * 40 + "\n")
            
            significant_tests = [name for name, result in statistical_tests.items() 
                               if result['significant']]
            
            if significant_tests:
                f.write("í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ë¥¼ ë³´ì¸ ì§€í‘œë“¤:\n")
                for test_name in significant_tests:
                    result = statistical_tests[test_name]
                    f.write(f"  - {test_name}: t = {result['t_statistic']:.3f}, "
                           f"p = {result['p_value']:.4f}, "
                           f"íš¨ê³¼í¬ê¸° = {result['effect_size']:.3f}\n")
            else:
                f.write("í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ë¥¼ ë³´ì¸ ì§€í‘œê°€ ì—†ìŠµë‹ˆë‹¤.\n")
            
            f.write("\nğŸ’¡ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­\n")
            f.write("-" * 40 + "\n")
            
            # ì„±ëŠ¥ í–¥ìƒë„ ê³„ì‚°
            improvements = {}
            significant_improvements = []
            
            for metric, metric_data in analysis_results.items():
                if 'proposed_digital_pheromone' in metric_data:
                    proposed_mean = metric_data['proposed_digital_pheromone']['mean']
                    
                    for method in self.baseline_methods:
                        if method in metric_data:
                            baseline_mean = metric_data[method]['mean']
                            if baseline_mean != 0:
                                improvement = ((proposed_mean - baseline_mean) / abs(baseline_mean)) * 100
                                improvements[f"{metric}_vs_{method}"] = improvement
                                
                                # í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ê°œì„  í™•ì¸
                                test_key = f"{metric}_{method}"
                                if test_key in statistical_tests and statistical_tests[test_key]['significant']:
                                    significant_improvements.append({
                                        'metric': metric,
                                        'baseline': method,
                                        'improvement': improvement,
                                        'p_value': statistical_tests[test_key]['p_value'],
                                        'effect_size': statistical_tests[test_key]['effect_size']
                                    })
            
            # ì „ì²´ ì„±ëŠ¥ í–¥ìƒë„ ë³´ê³ 
            if improvements:
                f.write("ì œì•ˆ ë°©ë²•ì˜ ì „ì²´ ì„±ëŠ¥ í–¥ìƒë„:\n")
                for key, improvement in improvements.items():
                    metric, baseline = key.replace('_vs_', ' vs ').split(' vs ')
                    significance = ""
                    test_key = f"{metric}_{baseline}"
                    if test_key in statistical_tests and statistical_tests[test_key]['significant']:
                        significance = " (í†µê³„ì  ìœ ì˜ì„± í™•ì¸)"
                    f.write(f"  - {metric} vs {baseline}: {improvement:+.2f}%{significance}\n")
            
            # ì£¼ìš” ì„±ê³¼ ìš”ì•½
            f.write("\nğŸ“Š ì£¼ìš” ì„±ê³¼ ìš”ì•½:\n")
            if significant_improvements:
                f.write(f"â€¢ {len(significant_improvements)}ê°œ ì§€í‘œì—ì„œ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì„±ëŠ¥ í–¥ìƒ í™•ì¸\n")
                
                # ê°€ì¥ í° ê°œì„  ì‚¬í•­ ê°•ì¡°
                best_improvement = max(significant_improvements, key=lambda x: abs(x['improvement']))
                f.write(f"â€¢ ìµœëŒ€ ê°œì„ : {best_improvement['metric']} ì§€í‘œì—ì„œ "
                       f"{best_improvement['improvement']:+.2f}% í–¥ìƒ "
                       f"(vs {best_improvement['baseline']})\n")
                
                # íš¨ê³¼ í¬ê¸° ë¶„ì„
                large_effects = [imp for imp in significant_improvements if abs(imp['effect_size']) > 0.8]
                if large_effects:
                    f.write(f"â€¢ {len(large_effects)}ê°œ ì§€í‘œì—ì„œ í° íš¨ê³¼ í¬ê¸° (|d| > 0.8) í™•ì¸\n")
            else:
                f.write("â€¢ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì„±ëŠ¥ ê°œì„ ì´ í™•ì¸ë˜ì§€ ì•ŠìŒ\n")
            
            # ê¶Œì¥ì‚¬í•­
            f.write("\nğŸ“ˆ ê¶Œì¥ì‚¬í•­:\n")
            if len(significant_improvements) >= 3:
                f.write("â€¢ ì œì•ˆëœ 4D ë””ì§€í„¸ í˜ë¡œëª¬ ë°©ë²•ì´ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìŒ\n")
                f.write("â€¢ ì‹¤ì œ ì‹œìŠ¤í…œ ì ìš©ì„ ìœ„í•œ ì¶”ê°€ ê²€ì¦ ê¶Œì¥\n")
            elif len(significant_improvements) > 0:
                f.write("â€¢ ì¼ë¶€ ì§€í‘œì—ì„œ ê°œì„  í™•ì¸, ì¶”ê°€ ìµœì í™” í•„ìš”\n")
                f.write("â€¢ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° ì•„í‚¤í…ì²˜ ê°œì„  ê¶Œì¥\n")
            else:
                f.write("â€¢ í˜„ì¬ êµ¬í˜„ì—ì„œëŠ” ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„ ëª…í™•í•œ ìš°ìœ„ ë¯¸í™•ì¸\n")
                f.write("â€¢ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì¬ê²€í†  ë° í›ˆë ¨ ì „ëµ ê°œì„  í•„ìš”\n")
                f.write("â€¢ ë” ë§ì€ ë°˜ë³µ ì‹¤í—˜ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ ê¶Œì¥\n")
            
            # ì‹¤í—˜ ë©”íƒ€ë°ì´í„°
            f.write("\nğŸ“ ì‹¤í—˜ ë©”íƒ€ë°ì´í„°:\n")
            total_experiments = self.num_runs * (len(self.baseline_methods) + 1)  # +1 for proposed method
            f.write(f"â€¢ ì´ ì‹¤í—˜ íšŸìˆ˜: {total_experiments}íšŒ\n")
            f.write(f"â€¢ ë¹„êµ ì§€í‘œ ìˆ˜: {len(analysis_results)}ê°œ\n")
            f.write(f"â€¢ í†µê³„ ê²€ì • ìˆ˜: {len(statistical_tests)}ê°œ\n")
            f.write(f"â€¢ ë³´ê³ ì„œ ìƒì„± ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            
            f.write("\n" + "=" * 80 + "\n")
            f.write("ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ\n")
            f.write("=" * 80 + "\n")
        
        logger.info(f"ë¹„êµ ì‹¤í—˜ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_path}")
    
    def save_results_as_json(self, analysis_results: Dict, statistical_tests: Dict):
        """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        json_results = {
            'experiment_info': {
                'baseline_methods': self.baseline_methods,
                'num_runs': self.num_runs,
                'significance_level': self.significance_level,
                'generated_at': time.strftime('%Y-%m-%d %H:%M:%S')
            },
            'analysis_results': analysis_results,
            'statistical_tests': statistical_tests,
            'summary': self._generate_summary_statistics(analysis_results, statistical_tests)
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        json_path = os.path.join(self.results_dir, "comparison_results.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"JSON ê²°ê³¼ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {json_path}")
    
    def _generate_summary_statistics(self, analysis_results: Dict, statistical_tests: Dict) -> Dict:
        """ìš”ì•½ í†µê³„ ìƒì„±"""
        summary = {
            'total_metrics_analyzed': len(analysis_results),
            'total_statistical_tests': len(statistical_tests),
            'significant_tests': sum(1 for test in statistical_tests.values() if test['significant']),
            'methods_compared': list(set(
                method for metric_data in analysis_results.values() 
                for method in metric_data.keys()
            ))
        }
        
        # ì œì•ˆ ë°©ë²•ì˜ ì„±ëŠ¥ í–¥ìƒë„ ê³„ì‚°
        if any('proposed_digital_pheromone' in metric_data for metric_data in analysis_results.values()):
            improvements = {}
            for metric, metric_data in analysis_results.items():
                if 'proposed_digital_pheromone' in metric_data:
                    proposed_mean = metric_data['proposed_digital_pheromone']['mean']
                    
                    for method in self.baseline_methods:
                        if method in metric_data:
                            baseline_mean = metric_data[method]['mean']
                            if baseline_mean != 0:
                                improvement = ((proposed_mean - baseline_mean) / abs(baseline_mean)) * 100
                                improvements[f"{metric}_vs_{method}"] = improvement
            
            summary['performance_improvements'] = improvements
        
        return summary
    
    def save_experiment_summary(self, analysis_results: Dict, statistical_tests: Dict):
        """ì‹¤í—˜ ìš”ì•½ í†µê³„ë¥¼ ê°„ê²°í•œ í˜•íƒœë¡œ ì €ì¥"""
        summary_data = []
        
        for metric, metric_data in analysis_results.items():
            for method, stats in metric_data.items():
                # í†µê³„ì  ìœ ì˜ì„± í™•ì¸
                is_significant = False
                p_value = None
                effect_size = None
                
                if method != 'proposed_digital_pheromone':
                    test_key = f"{metric}_{method}"
                    if test_key in statistical_tests:
                        is_significant = statistical_tests[test_key]['significant']
                        p_value = statistical_tests[test_key]['p_value']
                        effect_size = statistical_tests[test_key]['effect_size']
                
                summary_data.append({
                    'metric': metric,
                    'method': method,
                    'mean': stats['mean'],
                    'std': stats['std'],
                    'min': stats['min'],
                    'max': stats['max'],
                    'is_proposed': method == 'proposed_digital_pheromone',
                    'is_significant': is_significant,
                    'p_value': p_value,
                    'effect_size': effect_size
                })
        
        # DataFrameìœ¼ë¡œ ë³€í™˜ ë° ì €ì¥
        df_summary = pd.DataFrame(summary_data)
        summary_path = os.path.join(self.results_dir, "experiment_summary.csv")
        df_summary.to_csv(summary_path, index=False)
        
        # ì œì•ˆ ë°©ë²• vs ê¸°ì¤€ì„  ë¹„êµ í…Œì´ë¸” ìƒì„±
        if 'proposed_digital_pheromone' in df_summary['method'].values:
            comparison_data = []
            proposed_data = df_summary[df_summary['method'] == 'proposed_digital_pheromone']
            
            for _, proposed_row in proposed_data.iterrows():
                metric = proposed_row['metric']
                baseline_data = df_summary[
                    (df_summary['metric'] == metric) & 
                    (df_summary['method'] != 'proposed_digital_pheromone')
                ]
                
                for _, baseline_row in baseline_data.iterrows():
                    improvement = 0
                    if baseline_row['mean'] != 0:
                        improvement = ((proposed_row['mean'] - baseline_row['mean']) / abs(baseline_row['mean'])) * 100
                    
                    comparison_data.append({
                        'metric': metric,
                        'baseline_method': baseline_row['method'],
                        'proposed_mean': proposed_row['mean'],
                        'baseline_mean': baseline_row['mean'],
                        'improvement_percent': improvement,
                        'is_significant': baseline_row['is_significant'],
                        'p_value': baseline_row['p_value'],
                        'effect_size': baseline_row['effect_size']
                    })
            
            df_comparison = pd.DataFrame(comparison_data)
            comparison_path = os.path.join(self.results_dir, "method_comparison.csv")
            df_comparison.to_csv(comparison_path, index=False)
            
            logger.info(f"ì‹¤í—˜ ìš”ì•½ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {summary_path}")
            logger.info(f"ë°©ë²• ë¹„êµ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {comparison_path}")
    
    def save_comparison_training_summary_to_file(self, all_results: List[Dict], analysis_results: Dict, statistical_tests: Dict):
        """ë² ì´ìŠ¤ë¼ì¸ê³¼ ì œì•ˆëœ ëª¨ë¸ì˜ ë¹„êµ ê²°ê³¼ë¥¼ í¬í•¨í•œ training_summary.txt ìƒì„±"""
        summary_path = os.path.join(self.results_dir, 'training_summary.txt')
        
        # ê²°ê³¼ë¥¼ ë©”ì„œë“œë³„ë¡œ ë¶„ë¥˜
        method_results = {}
        for result in all_results:
            method = result['method']
            if method not in method_results:
                method_results[method] = []
            method_results[method].append(result)
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("4D ë””ì§€í„¸ í˜ë¡œëª¬ MAS ë¹„êµ ì‹¤í—˜ í›ˆë ¨ ìš”ì•½ ë³´ê³ ì„œ\n")
            f.write("=" * 80 + "\n\n")
            
            # ì‹¤í—˜ ê°œìš”
            f.write("ğŸ”¬ ë¹„êµ ì‹¤í—˜ ê°œìš”\n")
            f.write("-" * 40 + "\n")
            f.write(f"ì œì•ˆ ë°©ë²•: 4D ë””ì§€í„¸ í˜ë¡œëª¬ + ë¶„ì‚° ì–´í…ì…˜ ë„¤íŠ¸ì›Œí¬\n")
            f.write(f"ê¸°ì¤€ì„  ë°©ë²•: {', '.join(self.baseline_methods)}\n")
            f.write(f"ë°˜ë³µ ì‹¤í–‰ íšŸìˆ˜: {self.num_runs}íšŒ\n")
            f.write(f"ì´ ì‹¤í—˜ ìˆ˜: {len(all_results)}ê°œ\n")
            f.write(f"ìœ ì˜ìˆ˜ì¤€: Î± = {self.significance_level}\n\n")
            
            # ì œì•ˆ ë°©ë²• ìš”ì•½
            if 'proposed_digital_pheromone' in method_results:
                proposed_results = method_results['proposed_digital_pheromone']
                f.write("ğŸš€ ì œì•ˆ ë°©ë²• ì„±ëŠ¥ ìš”ì•½\n")
                f.write("-" * 40 + "\n")
                
                # í‰ê·  ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
                avg_metrics = self._calculate_average_metrics(proposed_results)
                f.write(f"ì •ë³´ ì „ë‹¬ íš¨ìœ¨ì„±: {avg_metrics.get('information_transfer_efficiency', 0):.4f} Â± {self._calculate_std_metrics(proposed_results, 'information_transfer_efficiency'):.4f}\n")
                f.write(f"í•™ìŠµ ìˆ˜ë ´ ì—í¬í¬: {avg_metrics.get('learning_convergence_epochs', 0):.1f} Â± {self._calculate_std_metrics(proposed_results, 'learning_convergence_epochs'):.1f}\n")
                f.write(f"í†µì‹  ì˜¤ë²„í—¤ë“œ: {avg_metrics.get('communication_overhead', 0):.4f} Â± {self._calculate_std_metrics(proposed_results, 'communication_overhead'):.4f}\n")
                f.write(f"ë„¤íŠ¸ì›Œí¬ ë¶€í•˜: {avg_metrics.get('network_load', 0):.4f} Â± {self._calculate_std_metrics(proposed_results, 'network_load'):.4f}\n")
                f.write(f"Shannon ì—”íŠ¸ë¡œí”¼: {avg_metrics.get('shannon_entropy', 0):.4f} Â± {self._calculate_std_metrics(proposed_results, 'shannon_entropy'):.4f}\n")
                f.write(f"ì„±ê³µë¥ : {avg_metrics.get('success_rate', 0):.4f} Â± {self._calculate_std_metrics(proposed_results, 'success_rate'):.4f}\n")
                f.write(f"í‰ê·  ë³´ìƒ: {avg_metrics.get('reward', 0):.4f} Â± {self._calculate_std_metrics(proposed_results, 'reward'):.4f}\n\n")
            
            # ë² ì´ìŠ¤ë¼ì¸ ë°©ë²•ë“¤ ìš”ì•½
            f.write("ğŸ¯ ê¸°ì¤€ì„  ë°©ë²• ì„±ëŠ¥ ìš”ì•½\n")
            f.write("-" * 40 + "\n")
            
            for method in self.baseline_methods:
                if method in method_results:
                    baseline_results = method_results[method]
                    f.write(f"\n[{method.upper()}]\n")
                    
                    avg_metrics = self._calculate_average_metrics(baseline_results)
                    f.write(f"  ì •ë³´ ì „ë‹¬ íš¨ìœ¨ì„±: {avg_metrics.get('information_transfer_efficiency', 0):.4f} Â± {self._calculate_std_metrics(baseline_results, 'information_transfer_efficiency'):.4f}\n")
                    f.write(f"  í•™ìŠµ ìˆ˜ë ´ ì—í¬í¬: {avg_metrics.get('learning_convergence_epochs', 0):.1f} Â± {self._calculate_std_metrics(baseline_results, 'learning_convergence_epochs'):.1f}\n")
                    f.write(f"  í†µì‹  ì˜¤ë²„í—¤ë“œ: {avg_metrics.get('communication_overhead', 0):.4f} Â± {self._calculate_std_metrics(baseline_results, 'communication_overhead'):.4f}\n")
                    f.write(f"  ë„¤íŠ¸ì›Œí¬ ë¶€í•˜: {avg_metrics.get('network_load', 0):.4f} Â± {self._calculate_std_metrics(baseline_results, 'network_load'):.4f}\n")
                    f.write(f"  Shannon ì—”íŠ¸ë¡œí”¼: {avg_metrics.get('shannon_entropy', 0):.4f} Â± {self._calculate_std_metrics(baseline_results, 'shannon_entropy'):.4f}\n")
                    f.write(f"  ì„±ê³µë¥ : {avg_metrics.get('success_rate', 0):.4f} Â± {self._calculate_std_metrics(baseline_results, 'success_rate'):.4f}\n")
                    f.write(f"  í‰ê·  ë³´ìƒ: {avg_metrics.get('reward', 0):.4f} Â± {self._calculate_std_metrics(baseline_results, 'reward'):.4f}\n")
            
            f.write("\n")
            
            # ì„±ëŠ¥ ë¹„êµ ë¶„ì„
            f.write("ğŸ“Š ì„±ëŠ¥ ë¹„êµ ë¶„ì„\n")
            f.write("-" * 40 + "\n")
            
            if 'proposed_digital_pheromone' in analysis_results:
                for metric, metric_data in analysis_results.items():
                    if 'proposed_digital_pheromone' in metric_data:
                        f.write(f"\n[{metric.upper()}]\n")
                        proposed_mean = metric_data['proposed_digital_pheromone']['mean']
                        f.write(f"  ì œì•ˆ ë°©ë²•: {proposed_mean:.4f}\n")
                        
                        for method in self.baseline_methods:
                            if method in metric_data:
                                baseline_mean = metric_data[method]['mean']
                                improvement = 0
                                if baseline_mean != 0:
                                    improvement = ((proposed_mean - baseline_mean) / abs(baseline_mean)) * 100
                                
                                # í†µê³„ì  ìœ ì˜ì„± í™•ì¸
                                test_key = f"{metric}_{method}"
                                significance = ""
                                if test_key in statistical_tests and statistical_tests[test_key]['significant']:
                                    p_value = statistical_tests[test_key]['p_value']
                                    significance = f" (p={p_value:.4f}, í†µê³„ì  ìœ ì˜ì„± âœ…)"
                                else:
                                    significance = " (í†µê³„ì  ìœ ì˜ì„± âŒ)"
                                
                                f.write(f"  vs {method}: {baseline_mean:.4f} â†’ {improvement:+.2f}% ì°¨ì´{significance}\n")
            
            # í†µê³„ì  ìœ ì˜ì„± ê²€ì • ìš”ì•½
            f.write("\nğŸ”¬ í†µê³„ì  ìœ ì˜ì„± ê²€ì • ìš”ì•½\n")
            f.write("-" * 40 + "\n")
            
            significant_tests = [name for name, result in statistical_tests.items() if result['significant']]
            total_tests = len(statistical_tests)
            
            f.write(f"ì „ì²´ í†µê³„ ê²€ì •: {total_tests}ê°œ\n")
            f.write(f"í†µê³„ì  ìœ ì˜í•œ ì°¨ì´: {len(significant_tests)}ê°œ ({len(significant_tests)/max(total_tests,1)*100:.1f}%)\n\n")
            
            if significant_tests:
                f.write("í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ê°œì„  ì§€í‘œ:\n")
                for test_name in significant_tests[:10]:  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
                    result = statistical_tests[test_name]
                    metric, baseline = test_name.split('_', 1)
                    f.write(f"  â€¢ {metric} vs {baseline}: ")
                    f.write(f"t={result['t_statistic']:.3f}, p={result['p_value']:.4f}, ")
                    f.write(f"íš¨ê³¼í¬ê¸°={result['effect_size']:.3f}\n")
            else:
                f.write("í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì„±ëŠ¥ ì°¨ì´ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n")
            
            # ì „ì²´ ì„±ëŠ¥ í–¥ìƒë„ ìš”ì•½
            f.write("\nğŸ“ˆ ì „ì²´ ì„±ëŠ¥ í–¥ìƒë„ ìš”ì•½\n")
            f.write("-" * 40 + "\n")
            
            significant_improvements = []
            all_improvements = []
            
            for metric, metric_data in analysis_results.items():
                if 'proposed_digital_pheromone' in metric_data:
                    proposed_mean = metric_data['proposed_digital_pheromone']['mean']
                    
                    for method in self.baseline_methods:
                        if method in metric_data:
                            baseline_mean = metric_data[method]['mean']
                            if baseline_mean != 0:
                                improvement = ((proposed_mean - baseline_mean) / abs(baseline_mean)) * 100
                                all_improvements.append(improvement)
                                
                                test_key = f"{metric}_{method}"
                                if test_key in statistical_tests and statistical_tests[test_key]['significant']:
                                    significant_improvements.append({
                                        'metric': metric,
                                        'baseline': method,
                                        'improvement': improvement
                                    })
            
            if all_improvements:
                avg_improvement = np.mean(all_improvements)
                f.write(f"ì „ì²´ í‰ê·  ì„±ëŠ¥ í–¥ìƒ: {avg_improvement:+.2f}%\n")
                
                positive_improvements = [imp for imp in all_improvements if imp > 0]
                if positive_improvements:
                    f.write(f"ê°œì„ ëœ ì§€í‘œ ë¹„ìœ¨: {len(positive_improvements)}/{len(all_improvements)} ({len(positive_improvements)/len(all_improvements)*100:.1f}%)\n")
                
                if significant_improvements:
                    sig_avg = np.mean([imp['improvement'] for imp in significant_improvements])
                    f.write(f"í†µê³„ì  ìœ ì˜í•œ ê°œì„  í‰ê· : {sig_avg:+.2f}%\n")
            
            # ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­
            f.write("\nğŸ’¡ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­\n")
            f.write("-" * 40 + "\n")
            
            if len(significant_improvements) >= 3:
                f.write("âœ… ì œì•ˆëœ 4D ë””ì§€í„¸ í˜ë¡œëª¬ ë°©ë²•ì´ ë‹¤ìˆ˜ ì§€í‘œì—ì„œ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì…ë‹ˆë‹¤.\n")
                f.write("â€¢ ì—°êµ¬ ê°€ì„¤ì´ ì‹¤í—˜ì ìœ¼ë¡œ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.\n")
                f.write("â€¢ ì‹¤ì œ ì‹œìŠ¤í…œ ì ìš©ì„ ìœ„í•œ ì¶”ê°€ ê²€ì¦ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n")
            elif len(significant_improvements) > 0:
                f.write("âš ï¸ ì¼ë¶€ ì§€í‘œì—ì„œ í†µê³„ì  ê°œì„ ì´ í™•ì¸ë˜ì—ˆìœ¼ë‚˜ ì „ì²´ì ì¸ ìš°ìœ„ëŠ” ì œí•œì ì…ë‹ˆë‹¤.\n")
                f.write("â€¢ ì¶”ê°€ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n")
                f.write("â€¢ ë” ë§ì€ ë°˜ë³µ ì‹¤í—˜ìœ¼ë¡œ ê²°ê³¼ì˜ ì‹ ë¢°ì„±ì„ ë†’ì´ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n")
            else:
                f.write("âŒ í˜„ì¬ ì„¤ì •ì—ì„œëŠ” ì œì•ˆ ë°©ë²•ì˜ ëª…í™•í•œ ìš°ìœ„ê°€ í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n")
                f.write("â€¢ ëª¨ë¸ ì•„í‚¤í…ì²˜ì˜ ì¬ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n")
                f.write("â€¢ í›ˆë ¨ ì „ëµ ë° ì‹¤í—˜ ì„¤ê³„ì˜ ê°œì„ ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n")
            
            # ì‹¤í—˜ ë©”íƒ€ë°ì´í„°
            f.write(f"\nğŸ“ ì‹¤í—˜ ë©”íƒ€ë°ì´í„°\n")
            f.write("-" * 40 + "\n")
            f.write(f"ì‹¤í—˜ ì™„ë£Œ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ì´ ë¶„ì„ëœ ì§€í‘œ: {len(analysis_results)}ê°œ\n")
            f.write(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.results_dir}\n")
            
            f.write("\n" + "=" * 80 + "\n")
            f.write("ë¹„êµ ì‹¤í—˜ í›ˆë ¨ ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ\n")
            f.write("=" * 80 + "\n")
        
        logger.info(f"ë¹„êµ ì‹¤í—˜ í›ˆë ¨ ìš”ì•½ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {summary_path}")
    
    def _calculate_average_metrics(self, results: List[Dict]) -> Dict:
        """ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì—ì„œ í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°"""
        metrics_sum = {}
        metrics_count = {}
        
        for result in results:
            # ë‹¤ì–‘í•œ ê²½ë¡œì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ
            metrics_sources = [
                result.get('summary', {}).get('research_metrics', {}),
                result.get('training_summary', {}).get('research_metrics', {}),
                result.get('training_summary', {}).get('performance_analysis', {}),
                result
            ]
            
            for source in metrics_sources:
                if isinstance(source, dict):
                    for key, value in source.items():
                        if isinstance(value, (int, float, np.generic)):
                            if key not in metrics_sum:
                                metrics_sum[key] = 0
                                metrics_count[key] = 0
                            metrics_sum[key] += float(value)
                            metrics_count[key] += 1
        
        return {key: metrics_sum[key] / max(metrics_count[key], 1) for key in metrics_sum}
    
    def _calculate_std_metrics(self, results: List[Dict], metric_name: str) -> float:
        """íŠ¹ì • ë©”íŠ¸ë¦­ì˜ í‘œì¤€í¸ì°¨ ê³„ì‚°"""
        values = []
        
        for result in results:
            value = 0.0
            
            # ë‹¤ì–‘í•œ ê²½ë¡œì—ì„œ ë©”íŠ¸ë¦­ ê²€ìƒ‰
            if 'summary' in result and 'research_metrics' in result['summary']:
                if metric_name in result['summary']['research_metrics']:
                    value = result['summary']['research_metrics'][metric_name]
            elif metric_name in result.get('training_summary', {}).get('research_metrics', {}):
                value = result['training_summary']['research_metrics'][metric_name]
            elif metric_name in result.get('training_summary', {}).get('performance_analysis', {}):
                perf_metric = result['training_summary']['performance_analysis'][metric_name]
                if isinstance(perf_metric, dict):
                    value = perf_metric.get('final', 0)
                else:
                    value = perf_metric if perf_metric is not None else 0.0
            elif metric_name in result:
                value = result[metric_name]

            if isinstance(value, (int, float, np.generic)):
                values.append(float(value))
        
        return np.std(values) if values else 0.0

def main():
    parser = argparse.ArgumentParser(description="Run Digital Pheromone MAS Comparison Experiment")
    parser.add_argument('--config', type=str, default='config/experiment_config.yaml',
                        help='Path to the experiment configuration file.')
    args = parser.parse_args()
    
    # ë¹„êµ ì‹¤í—˜ ì‹¤í–‰
    comparison_runner = ComparisonExperimentRunner(config_path=args.config)
    results = comparison_runner.run_comparison_experiment()
    
    logger.info("ë¹„êµ ì‹¤í—˜ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
