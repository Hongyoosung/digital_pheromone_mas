import os
import yaml
import torch
import numpy as np
import ray
from typing import Dict, List
import time
import logging
from tqdm import tqdm
import wandb
import argparse
import torch.nn.functional as F
import pickle
import os
import json
import psutil
import traceback

from src.core.agent import DistributedAgent
from src.core.pheromone_vector import PheromoneField, PheromoneVector
from src.models.diffusion_model import TemporalDiffusionModel
from src.models.attention_network import DistributedAttentionRouter
from src.utils.normalization import PheromoneNormalizer
from src.utils.metrics import MetricsTracker
from src.utils.visualization import ExperimentVisualizer
from src.utils.memory_manager import MemoryManager, BatchMemoryOptimizer

# CUDA ë¡œê¹… ê°„ì†Œí™”
os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
os.environ['NCCL_DEBUG'] = 'WARN'
os.environ['TORCH_SHOW_CPP_STACKTRACES'] = '0'

logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

torch.backends.cudnn.benchmark = False
if hasattr(torch._C, '_set_print_stacktraces_on_fatal_signal'):
    torch._C._set_print_stacktraces_on_fatal_signal(False)

class ExperimentRunner:
    def __init__(self, config_path: str):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
            
        requested_device = self.config['experiment']['device']
        if 'cuda' in requested_device and not torch.cuda.is_available():
            logger.warning("CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. CPUë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            self.device = torch.device('cpu')
            self.config['experiment']['device'] = 'cpu'
        else:
            self.device = torch.device(requested_device)
            if 'cuda' in requested_device:
                torch.cuda.empty_cache()
                if logger.isEnabledFor(logging.INFO):
                    logger.info(f"GPU ë””ë°”ì´ìŠ¤ í™•ì¸ë¨: {torch.cuda.get_device_name(0)}")
        
        self.start_time = time.time()
        self.learning_history = {
            'attention_loss': [], 'diffusion_loss': [], 'consistency_loss': [],
            'total_loss': [], 'learning_rates': {'attention': [], 'diffusion': []},
            'convergence_metrics': [], 'training_steps': [], 'pheromone_decay_rate': [],
            'pheromone_deposit_rate': [], 'pheromone_evaporation_rate': []
        }
        self.best_loss = float('inf')
        self.patience_counter = 0
        self.early_stopping_patience = 50
        self.previous_pheromone_field = None
        
        self.setup_experiment()
        
        # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì¶”ê°€
        from src.core.trainer import PheromoneNetworkTrainer
        self.trainer = None
        
    def setup_experiment(self):
        if ray.is_initialized():
            ray.shutdown()
        
        ray_config = {
            "num_cpus": min(4, os.cpu_count()),
            "object_store_memory": 2 * 1024**3,
        }
        if "cuda" in self.config['experiment']['device']:
            ray_config["num_gpus"] = 1
        
        ray.init(**ray_config)
        
        if self.device.type == 'cuda':
            torch.cuda.set_device(self.device)
            
        self.diffusion_model = TemporalDiffusionModel(decay_factor=self.config['pheromone']['decay_rate']).to(self.device)
        self.attention_router = DistributedAttentionRouter(embed_dim=self.config['attention']['embed_dim'], num_heads=self.config['attention']['num_heads']).to(self.device)
        self.diffusion_model.eval()
        self.attention_router.eval()
        
        self.normalizer = PheromoneNormalizer()
        self.metrics_tracker = MetricsTracker()
        log_dir = self.config['experiment']['log_dir']
        os.makedirs(log_dir, exist_ok=True)
        self.visualizer = ExperimentVisualizer(log_dir)
        
        memory_config = self.config.get('memory_management', {})
        self.memory_manager = MemoryManager(
            max_memory_percent=memory_config.get('max_memory_percent', 85.0),
            warning_memory_percent=memory_config.get('warning_memory_percent', 75.0)
        )
        
        map_size = tuple(self.config['environment']['map_size'])
        self.pheromone_field = PheromoneField(map_size, self.config['pheromone']['decay_rate'])
        self.agents = self.create_agents()
        self.batch_size = min(len(self.agents), 4)
        
        if self.config['monitoring'].get('use_wandb', False):
            wandb.init(project="digital_pheromone_mas", config=self.config)
            
    def create_agents(self) -> List:
        num_agents = self.config['environment']['num_agents']
        p_dims = self.config['pheromone']['dimensions']
        pheromone_dim = sum(p_dims.values())
        
        agent_options = {"num_cpus": 0.25}
        if "cuda" in self.config['experiment']['device'] and num_agents > 0:
            agent_options["num_gpus"] = 1.0 / num_agents
            
        AgentActor = DistributedAgent.options(**agent_options)
        agents = []
        for i in range(num_agents):
            agent_config = {
                'map_size': self.config['environment']['map_size'],
                'pheromone_dim': pheromone_dim,
                'num_agents': num_agents,
                'device': self.config['experiment']['device']
            }
            agents.append(AgentActor.remote(i, agent_config))
        return agents
        
    def run_timestep(self, t: int) -> Dict:
        timestep_metrics = {}
        comp_load = {}

        start_time = time.perf_counter()
        field_dict = {pos: pheromones for pos, pheromones in self.pheromone_field.field.items()}
        
        perception_futures = [agent.perceive_pheromones.remote(field_dict) for agent in self.agents]
        encoded_pheromones_list = ray.get(perception_futures)
        action_futures = [agent.decide_action.remote(encoded) for agent, encoded in zip(self.agents, encoded_pheromones_list)]
        actions = ray.get(action_futures)
        comp_load['action_decision'] = time.perf_counter() - start_time

        start_time = time.perf_counter()
        environment_state = {'field_density': len(self.pheromone_field.field), 'timestep': t}
        execution_futures = [agent.execute_action.remote(action, environment_state) for agent, action in zip(self.agents, actions)]
        action_results = ray.get(execution_futures)
        comp_load['action_execution'] = time.perf_counter() - start_time
        
        successful_actions = sum(1 for result in action_results if result['success'])
        total_reward = sum(result['reward'] for result in action_results)
        timestep_metrics['success_rate'] = successful_actions / len(action_results) if action_results else 0
        timestep_metrics['reward'] = total_reward / len(action_results) if action_results else 0
        
        start_time = time.perf_counter()
        agent_futures = [agent.emit_pheromone.remote() for agent in self.agents]
        new_pheromones = ray.get(agent_futures)
        comp_load['pheromone_emission'] = time.perf_counter() - start_time

        start_time = time.perf_counter()
        agent_states_futures = [agent.get_state.remote() for agent in self.agents]
        agent_states = ray.get(agent_states_futures)
        for i, pheromone in enumerate(new_pheromones):
            agent_state = agent_states[i]
            pos = tuple(int(p) for p in agent_state['position'])
            self.pheromone_field.deposit(pos, pheromone)
        comp_load['pheromone_deposit'] = time.perf_counter() - start_time

        start_time = time.perf_counter()
        
        # í™•ì‚°/ê°ì‡  ì „ í•„ë“œ ìƒíƒœ ì €ì¥ (í˜ë¡œëª¬ ë™ì—­í•™ ë©”íŠ¸ë¦­ìš©)
        field_before = self.create_field_tensor().cpu().numpy() if hasattr(self, 'previous_pheromone_field') else None
        
        self.pheromone_field.diffuse(radius=2)
        min_magnitude = self.config['pheromone']['lifecycle']['min_magnitude_threshold']
        max_lifetime = self.config['pheromone']['lifecycle']['max_lifetime_seconds']
        self.pheromone_field.decay_all(min_magnitude, max_lifetime)
        
        # í™•ì‚°/ê°ì‡  í›„ í•„ë“œ ìƒíƒœ ì €ì¥
        field_after = self.create_field_tensor().cpu().numpy()
        
        # í˜ë¡œëª¬ ë™ì—­í•™ ë©”íŠ¸ë¦­ ê³„ì‚°
        if field_before is not None:
            pheromone_dynamics = self.metrics_tracker.compute_pheromone_dynamics_metrics(field_after, field_before)
            timestep_metrics.update(pheromone_dynamics)
        
        # ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•´ í˜„ì¬ ìƒíƒœ ì €ì¥
        self.previous_pheromone_field = field_after
        
        comp_load['diffusion_decay'] = time.perf_counter() - start_time
        timestep_metrics['computation_overhead'] = comp_load

        if t > 0 and t % self.config['hyperparameters']['communication_period'][0] == 0:
            comm_metrics = self.execute_communication_round()
            self.metrics_tracker.update(communication_overhead=[comm_metrics])

        if (
            self.config['hyperparameters'].get('train_networks', True) and 
            t > 0 and t % self.config['hyperparameters'].get('training_frequency', 10) == 0
        ):
            field_tensor = self.create_field_tensor()
            training_metrics = self.execute_network_training(t, encoded_pheromones_list, field_tensor, action_results)
            timestep_metrics.update(training_metrics)
            
        field_tensor = self.create_field_tensor()
        if field_tensor.nelement() > 0:
            field_numpy = field_tensor.cpu().numpy()
            timestep_metrics['shannon_entropy'] = self.metrics_tracker.compute_shannon_entropy(field_numpy)
        
        return timestep_metrics
        
    def create_field_tensor(self) -> torch.Tensor:
        H, W = self.config['environment']['map_size']
        p_dims = self.config['pheromone']['dimensions']
        dim_count = sum(p_dims.values())
        field_tensor = torch.zeros(1, dim_count, H, W, device=self.device)
        
        for pos, pheromones in self.pheromone_field.field.items():
            if pheromones:
                x, y = pos
                zero_vector = PheromoneVector.zeros(p_dims)
                aggregated = sum(pheromones, zero_vector)
                field_tensor[0, :, x, y] = aggregated.to_tensor(device=self.device)
        return field_tensor
        
    def execute_communication_round(self) -> Dict:
        """ì‹¤ì œ ì—ì´ì „íŠ¸ ê°„ ë©”ì‹œì§€ êµí™˜ êµ¬í˜„"""
        start_time = time.perf_counter()
        
        # í†µì‹ í•  ì—ì´ì „íŠ¸ ìŒ ì„ íƒ (ì—°êµ¬ ê³„íšì„œ 4.1í•­ ìš”êµ¬ì‚¬í•­)
        num_agents = len(self.agents)
        communication_pairs = []
        
        # ë” í˜„ì‹¤ì ì¸ í†µì‹  íŒ¨í„´ - ëª¨ë“  ì—ì´ì „íŠ¸ê°€ í•­ìƒ í†µì‹ í•˜ì§€ ì•ŠìŒ
        active_agents = np.random.choice(num_agents, size=max(1, int(num_agents * 0.7)), replace=False)
        
        for i in active_agents:
            # í†µì‹  ë¹ˆë„ ê°ì†Œ - 1-2ê°œ ì—ì´ì „íŠ¸ì™€ë§Œ í†µì‹  (ê¸°ì¡´ 1-3ê°œì—ì„œ ê°ì†Œ)
            max_targets = min(2, num_agents - 1)
            num_targets = np.random.randint(1, max_targets + 1)
            
            # ê±°ë¦¬ ê¸°ë°˜ í†µì‹  í™•ë¥  ì¶”ê°€ (ê°€ê¹Œìš´ ì—ì´ì „íŠ¸ì™€ ë” ìì£¼ í†µì‹ )
            available_targets = [j for j in range(num_agents) if j != i]
            
            if len(available_targets) > 0:
                # í™•ë¥ ì  ì„ íƒ (ì™„ì „ ëœë¤ì´ ì•„ë‹Œ í¸í–¥ëœ ì„ íƒ)
                if len(available_targets) <= num_targets:
                    targets = available_targets
                else:
                    targets = np.random.choice(available_targets, size=num_targets, replace=False)
                
                for target in targets:
                    # 80% í™•ë¥ ë¡œë§Œ ì‹¤ì œ í†µì‹  ë°œìƒ
                    if np.random.random() < 0.8:
                        communication_pairs.append((i, target))
        
        # ì‹¤ì œ ë©”ì‹œì§€ ì „ì†¡ ë° ìˆ˜ì‹ 
        messages = []
        total_bytes = 0
        
        for sender_idx, receiver_idx in communication_pairs:
            sender = self.agents[sender_idx]
            receiver = self.agents[receiver_idx]
            
            # ë©”ì‹œì§€ ë‚´ìš© ìƒì„± (í˜ë¡œëª¬ ì •ë³´, ìœ„ì¹˜, ìƒíƒœ ë“±)
            try:
                sender_state = ray.get(sender.get_state.remote())
                message = {
                    'type': 'pheromone_info',
                    'sender_id': sender_idx,
                    'position': sender_state['position'],
                    'resources': sender_state['resources'],
                    'emotion_state': sender_state['emotion_state'],
                    'timestamp': time.time()
                }
                
                # ë©”ì‹œì§€ ì „ì†¡ (Ray Actor ë©”ì„œë“œ í˜¸ì¶œ)
                message_result = ray.get(sender.communicate.remote(receiver_idx, message))
                ray.get(receiver.receive_message.remote(sender_idx, message))
                
                # ë©”ì‹œì§€ í¬ê¸° ê³„ì‚° (ì—°êµ¬ ê³„íšì„œ 4.1í•­ ìš”êµ¬ì‚¬í•­)
                message_size = len(str(message).encode('utf-8'))
                total_bytes += message_size
                
                # ìƒí˜¸ì‘ìš© ìœ í˜• ê²°ì • - í˜„ì‹¤ì  ê°ˆë“± ìš”ì†Œ ì¶”ê°€
                interaction_types = ['cooperation', 'communication', 'competition']
                interaction_weights = [0.5, 0.3, 0.2]  # 50% í˜‘ë ¥, 30% ì†Œí†µ, 20% ê²½ìŸ
                interaction_type = np.random.choice(interaction_types, p=interaction_weights)
                
                # ì‚¬íšŒì  ì—°ê²° ì—…ë°ì´íŠ¸ (ì–‘ë°©í–¥)
                ray.get(sender.update_social_connections.remote(receiver_idx, interaction_type, 0.1))
                ray.get(receiver.update_social_connections.remote(sender_idx, interaction_type, 0.1))
                
                messages.append({
                    'sender': sender_idx,
                    'receiver': receiver_idx,
                    'size': message_size,
                    'timestamp': time.time(),
                    'type': message['type'],
                    'interaction_type': interaction_type,
                    'status': 'success'  # ì„±ê³µì ìœ¼ë¡œ ì „ì†¡ë¨
                })
                
            except Exception as e:
                logger.warning(f"í†µì‹  ì‹¤íŒ¨ (ì—ì´ì „íŠ¸ {sender_idx} -> {receiver_idx}): {e}")
                # ì‹¤íŒ¨í•œ ë©”ì‹œì§€ë„ ê¸°ë¡
                messages.append({
                    'sender': sender_idx,
                    'receiver': receiver_idx,
                    'size': 0,
                    'timestamp': time.time(),
                    'type': 'failed',
                    'status': 'failed'
                })
        
        # í†µì‹  ì§€ì—°ì‹œê°„ ì¸¡ì • (ì—°êµ¬ ê³„íšì„œ 4.1í•­ ìš”êµ¬ì‚¬í•­)
        communication_time = time.perf_counter() - start_time
        
        # ë„¤íŠ¸ì›Œí¬ ë¶€í•˜ ê³„ì‚°
        communication_metrics = {
            'total_messages': len(messages),
            'total_bytes': total_bytes,
            'communication_time': communication_time,
            'avg_message_size': total_bytes / len(messages) if messages else 0,
            'message_rate': len(messages) / communication_time if communication_time > 0 else 0,
            'bandwidth_usage': (total_bytes * 8) / (communication_time * 1024 * 1024) if communication_time > 0 else 0  # Mbps
        }
        
        return self.metrics_tracker.track_communication_overhead(messages, communication_metrics)

    def execute_network_training(self, timestep: int, encoded_pheromones: List[torch.Tensor], 
                                pheromone_field: torch.Tensor, action_results: List[Dict]) -> Dict:
        """ì‹¤ì œ ì‹ ê²½ë§ í›ˆë ¨ êµ¬í˜„ (ì—°êµ¬ ê³„íšì„œ ì„¹ì…˜ 3.4 ìš”êµ¬ì‚¬í•­)"""
        # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” (ì²˜ìŒ ì‹¤í–‰ì‹œ)
        if self.trainer is None:
            from src.core.trainer import PheromoneNetworkTrainer
            self.trainer = PheromoneNetworkTrainer(self.config, self.device)
            
        # ì—ì´ì „íŠ¸ ì„ë² ë”© ìƒì„±
        agent_embeddings = None
        if encoded_pheromones and len(encoded_pheromones) > 0:
            # ì—ì´ì „íŠ¸ ì„ë² ë”©ì„ ë°°ì¹˜ë¡œ ë³€í™˜
            valid_embeddings = [p for p in encoded_pheromones if p.numel() > 0]
            if valid_embeddings:
                agent_embeddings = torch.stack(valid_embeddings)
        
        # ë³´ìƒ ê³„ì‚°
        rewards = torch.tensor([result['reward'] for result in action_results], device=self.device)
        
        # íŠ¸ë ˆì´ë„ˆë¥¼ ì‚¬ìš©í•œ í›ˆë ¨
        if agent_embeddings is not None and pheromone_field.numel() > 0:
            training_losses = self.trainer.train_step(agent_embeddings, pheromone_field, timestep, rewards)
            
            # íŠ¸ë ˆì´ë„ˆì˜ í•™ìŠµ íˆìŠ¤í† ë¦¬ë¥¼ ë©”ì¸ ì‹¤í—˜ì˜ íˆìŠ¤í† ë¦¬ì— í†µí•©
            trainer_history = self.trainer.training_history
            for key, values in trainer_history.items():
                if values:  # ê°’ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ
                    if key not in self.learning_history:
                        self.learning_history[key] = []
                    if isinstance(values[-1], (int, float)):
                        self.learning_history[key].append(values[-1])
            
            # íŠ¸ë ˆì´ë‹ ìŠ¤í… ì¶”ê°€
            self.learning_history['training_steps'].append(timestep)
            
            return training_losses
        else:
            return {'training_loss': 0.0}
    
    def create_field_tensor(self) -> torch.Tensor:
        H, W = self.config['environment']['map_size']
        p_dims = self.config['pheromone']['dimensions']
        dim_count = sum(p_dims.values())
        field_tensor = torch.zeros(1, dim_count, H, W, device=self.device)
        
        for pos, pheromones in self.pheromone_field.field.items():
            if pheromones:
                x, y = pos
                zero_vector = PheromoneVector.zeros(p_dims)
                aggregated = sum(pheromones, zero_vector)
                field_tensor[0, :, x, y] = aggregated.to_tensor(device=self.device)
        return field_tensor
    
    def run_experiment(self) -> Dict:
        logger.info(f"Starting experiment with {len(self.agents)} agents")
        results = {'metrics': [], 'config': self.config, 'start_time': time.time()}
        max_timesteps = self.config['environment']['max_timesteps']
        
        for t in tqdm(range(max_timesteps), desc="Running simulation"):
            if not self.memory_manager.should_continue_training():
                logger.error(f"ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ íƒ€ì„ìŠ¤í… {t}ì—ì„œ ì‹œë®¬ë ˆì´ì…˜ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
            
            timestep_metrics = self.run_timestep(t)
            self.metrics_tracker.update(**timestep_metrics)
            
            if t > 0 and t % self.config['monitoring']['log_frequency'] == 0:
                self.record_research_metrics(t)
                
                # ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¡œê¹… ì¶”ê°€
                log = self.generate_learning_log(t, timestep_metrics)
                self.metrics_tracker.update(system_metrics=[log['system_metrics']])

        results['summary'] = self.generate_training_summary()
        logger.info("í›ˆë ¨ ì™„ë£Œ - ê°œì„ ëœ ìš”ì•½ ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        summary_path = os.path.join(self.config['experiment']['log_dir'], 'training_summary.txt')
        self.save_training_summary_to_file(results['summary'], summary_path)
        
        ray.shutdown()
        return results
        
    def generate_learning_log(self, timestep: int, metrics: Dict) -> Dict:
        memory_info = psutil.virtual_memory()
        gpu_memory_info = None
        if torch.cuda.is_available() and self.device.type == 'cuda':
            gpu_memory_info = {
                'allocated': torch.cuda.memory_allocated(self.device) / 1024**3,
                'reserved': torch.cuda.memory_reserved(self.device) / 1024**3,
            }
        return {
            'timestep': timestep,
            'system_metrics': {
                'cpu_usage': psutil.cpu_percent(),
                'memory_usage': memory_info.percent,
                'gpu_memory': gpu_memory_info
            }
        }

    def record_research_metrics(self, timestep: int):
        try:
            # ì‹¤ì œ ì—ì´ì „íŠ¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            agent_metrics_futures = [agent.get_metrics.remote() for agent in self.agents]
            agent_metrics_list = ray.get(agent_metrics_futures)
            
            # í†µì‹  ì˜¤ë²„í—¤ë“œ ê³„ì‚° (ì‹¤ì œ ë°ì´í„°)
            total_messages = sum(m.get('messages_sent', 0) + m.get('messages_received', 0) for m in agent_metrics_list)
            total_bytes = sum(m.get('bytes_sent', 0) + m.get('bytes_received', 0) for m in agent_metrics_list)
            avg_computation_time = np.mean([m.get('computation_time', 0) for m in agent_metrics_list])
            
            communication_overhead = {
                'total_messages': total_messages,
                'total_bytes': total_bytes,
                'avg_computation_time': avg_computation_time
            }
            
            # ë„¤íŠ¸ì›Œí¬ ë¶€í•˜ ê³„ì‚°
            time_window = 60  # 60ì´ˆ ìœˆë„ìš°
            bandwidth_usage = (total_bytes * 8) / (time_window * 1024 * 1024) if time_window > 0 else 0  # Mbps
            network_load = {
                'bandwidth_usage': bandwidth_usage,
                'avg_computation_time': avg_computation_time,
                'load_balance_ratio': np.std([m.get('computation_time', 0) for m in agent_metrics_list])
            }
            
            # ì •ë³´ ì „ë‹¬ íš¨ìœ¨ì„± ê³„ì‚° (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)
            successful_actions = sum(1 for metrics in self.metrics_tracker.metrics_history.get('success_rate', []) if metrics > 0.5)
            total_actions = len(self.metrics_tracker.metrics_history.get('success_rate', []))
            info_transfer_efficiency = self.metrics_tracker.compute_information_transfer_efficiency(
                [], successful_actions, max(total_actions, 1)
            )
            
            # í•™ìŠµ ìˆ˜ë ´ ì—í¬í¬ ê³„ì‚° (ì‹¤ì œ ì†ì‹¤ íˆìŠ¤í† ë¦¬ ê¸°ë°˜)
            if hasattr(self, 'learning_history') and self.learning_history['total_loss']:
                learning_convergence_epochs = self.metrics_tracker.compute_learning_convergence_epochs(
                    self.learning_history['total_loss']
                )
            else:
                learning_convergence_epochs = 0
            
            self.metrics_tracker.update(
                communication_overhead=[communication_overhead],
                network_load=[network_load], 
                information_transfer_efficiency=[info_transfer_efficiency],
                learning_convergence_epochs=[learning_convergence_epochs]
            )
            
            # **ì‹œê°í™” ìƒì„± ì¶”ê°€** - 50ìŠ¤í…ë§ˆë‹¤ ì‹¤í–‰
            logger.info(f"íƒ€ì„ìŠ¤í… {timestep}: ì‹œê°í™” ìë£Œ ìƒì„± ì¤‘...")
            
            # 1. í˜ë¡œëª¬ í•„ë“œ ì‹œê°í™”
            field_tensor = self.create_field_tensor()
            if field_tensor.nelement() > 0:
                # í…ì„œë¥¼ ì˜¬ë°”ë¥¸ í˜•íƒœë¡œ ë³€í™˜: [1, total_dims, H, W] -> [4, H, W]
                tensor_shape = field_tensor.shape
                total_dims = tensor_shape[1] 
                H, W = self.config['environment']['map_size']
                
                # 4ì°¨ì›ìœ¼ë¡œ ë¶„í•  (behavior, emotion, social, context)
                dims = self.config['pheromone']['dimensions']
                behavior_dim = dims['behavior'] 
                emotion_dim = dims['emotion']
                social_dim = dims['social'] 
                context_dim = dims['context']
                
                field_numpy = field_tensor.cpu().numpy().squeeze(0)  # [total_dims, H, W]
                
                # 4ê°œ ì°¨ì›ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ í‰ê·  ê³„ì‚°
                pheromone_4d = np.zeros((4, H, W))
                start_idx = 0
                
                # Behavior dimension
                end_idx = start_idx + behavior_dim
                if end_idx <= total_dims:
                    pheromone_4d[0] = np.mean(field_numpy[start_idx:end_idx], axis=0)
                start_idx = end_idx
                
                # Emotion dimension
                end_idx = start_idx + emotion_dim
                if end_idx <= total_dims:
                    pheromone_4d[1] = np.mean(field_numpy[start_idx:end_idx], axis=0)
                start_idx = end_idx
                
                # Social dimension
                end_idx = start_idx + social_dim
                if end_idx <= total_dims:
                    pheromone_4d[2] = np.mean(field_numpy[start_idx:end_idx], axis=0)
                start_idx = end_idx
                
                # Context dimension
                end_idx = start_idx + context_dim
                if end_idx <= total_dims:
                    pheromone_4d[3] = np.mean(field_numpy[start_idx:end_idx], axis=0)
                
                self.visualizer.plot_pheromone_field(pheromone_4d, timestep, save=True)
            
            # 2. ì—ì´ì „íŠ¸ ìƒíƒœ ì‹œê°í™”
            agent_states_futures = [agent.get_state.remote() for agent in self.agents]
            agent_states = ray.get(agent_states_futures)
            self.visualizer.plot_agent_states(agent_states, timestep, save=True)
            
            # 3. í•™ìŠµ ëª¨ë‹ˆí„°ë§ ì‹œê°í™” (íŠ¸ë ˆì´ë„ˆê°€ ìˆëŠ” ê²½ìš°)
            if hasattr(self, 'trainer') and self.trainer:
                trainer_history = self.trainer.training_history
                # í†µí•©ëœ í•™ìŠµ íˆìŠ¤í† ë¦¬ì— ìˆ˜ë ´ ë©”íŠ¸ë¦­ ì¶”ê°€
                convergence_metrics = []
                if len(self.learning_history['total_loss']) > 1:
                    current_loss = self.learning_history['total_loss'][-1]
                    previous_losses = self.learning_history['total_loss'][:-1]
                    improvement_metrics = self.metrics_tracker.compute_loss_improvement_metrics(current_loss, previous_losses)
                    convergence_metrics.append({
                        'timestep': timestep,
                        'loss_improvement': improvement_metrics['loss_improvement'],
                        'relative_improvement': improvement_metrics['relative_improvement']
                    })
                
                learning_vis_data = {
                    **self.learning_history,
                    'convergence_metrics': convergence_metrics
                }
                self.visualizer.create_learning_monitoring_plots(learning_vis_data, timestep, save=True)
            
            # 4. í†µì‹  ë¶„ì„ ì‹œê°í™”
            comm_data = self.metrics_tracker.metrics_history.get('communication_overhead', [])
            if comm_data:
                self.visualizer.create_communication_analysis_plot(comm_data, timestep, save=True)
            
        except Exception as e:
            logger.error(f"ì—°êµ¬ ë©”íŠ¸ë¦­ ê¸°ë¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logger.error(traceback.format_exc())


            
            timestep_metrics = self.run_timestep(t)
            self.metrics_tracker.update(**timestep_metrics)
            
            if t > 0 and t % self.config['monitoring']['log_frequency'] == 0:
                self.record_research_metrics(t)
                
                # ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¡œê¹… ì¶”ê°€
                log = self.generate_learning_log(t, timestep_metrics)
                self.metrics_tracker.update(system_metrics=[log['system_metrics']])

        results['summary'] = self.generate_training_summary()
        logger.info("í›ˆë ¨ ì™„ë£Œ - ê°œì„ ëœ ìš”ì•½ ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        summary_path = os.path.join(self.config['experiment']['log_dir'], 'training_summary.txt')
        self.save_training_summary_to_file(results['summary'], summary_path)
        
        ray.shutdown()
        return results
        
    def generate_learning_log(self, timestep: int, metrics: Dict) -> Dict:
        memory_info = psutil.virtual_memory()
        gpu_memory_info = None
        if torch.cuda.is_available() and self.device.type == 'cuda':
            gpu_memory_info = {
                'allocated': torch.cuda.memory_allocated(self.device) / 1024**3,
                'reserved': torch.cuda.memory_reserved(self.device) / 1024**3,
            }
        return {
            'timestep': timestep,
            'system_metrics': {
                'cpu_usage': psutil.cpu_percent(),
                'memory_usage': memory_info.percent,
                'gpu_memory': gpu_memory_info
            }
        }

    def record_research_metrics(self, timestep: int):
        try:
            # ì‹¤ì œ ì—ì´ì „íŠ¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            agent_metrics_futures = [agent.get_metrics.remote() for agent in self.agents]
            agent_metrics_list = ray.get(agent_metrics_futures)
            
            # í†µì‹  ì˜¤ë²„í—¤ë“œ ê³„ì‚° (ì‹¤ì œ ë°ì´í„°)
            total_messages = sum(m.get('messages_sent', 0) + m.get('messages_received', 0) for m in agent_metrics_list)
            total_bytes = sum(m.get('bytes_sent', 0) + m.get('bytes_received', 0) for m in agent_metrics_list)
            avg_computation_time = np.mean([m.get('computation_time', 0) for m in agent_metrics_list])
            
            communication_overhead = {
                'total_messages': total_messages,
                'total_bytes': total_bytes,
                'avg_computation_time': avg_computation_time
            }
            
            # ë„¤íŠ¸ì›Œí¬ ë¶€í•˜ ê³„ì‚°
            time_window = 60  # 60ì´ˆ ìœˆë„ìš°
            bandwidth_usage = (total_bytes * 8) / (time_window * 1024 * 1024) if time_window > 0 else 0  # Mbps
            network_load = {
                'bandwidth_usage': bandwidth_usage,
                'avg_computation_time': avg_computation_time,
                'load_balance_ratio': np.std([m.get('computation_time', 0) for m in agent_metrics_list])
            }
            
            # ì •ë³´ ì „ë‹¬ íš¨ìœ¨ì„± ê³„ì‚° (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)
            successful_actions = sum(1 for metrics in self.metrics_tracker.metrics_history.get('success_rate', []) if metrics > 0.5)
            total_actions = len(self.metrics_tracker.metrics_history.get('success_rate', []))
            info_transfer_efficiency = self.metrics_tracker.compute_information_transfer_efficiency(
                [], successful_actions, max(total_actions, 1)
            )
            
            # í•™ìŠµ ìˆ˜ë ´ ì—í¬í¬ ê³„ì‚° (ì‹¤ì œ ì†ì‹¤ íˆìŠ¤í† ë¦¬ ê¸°ë°˜)
            if hasattr(self, 'learning_history') and self.learning_history['total_loss']:
                learning_convergence_epochs = self.metrics_tracker.compute_learning_convergence_epochs(
                    self.learning_history['total_loss']
                )
            else:
                learning_convergence_epochs = 0
            
            self.metrics_tracker.update(
                communication_overhead=[communication_overhead],
                network_load=[network_load], 
                information_transfer_efficiency=[info_transfer_efficiency],
                learning_convergence_epochs=[learning_convergence_epochs]
            )
            
            # **ì‹œê°í™” ìƒì„± ì¶”ê°€** - 50ìŠ¤í…ë§ˆë‹¤ ì‹¤í–‰
            logger.info(f"íƒ€ì„ìŠ¤í… {timestep}: ì‹œê°í™” ìë£Œ ìƒì„± ì¤‘...")
            
            # 1. í˜ë¡œëª¬ í•„ë“œ ì‹œê°í™”
            field_tensor = self.create_field_tensor()
            if field_tensor.nelement() > 0:
                # í…ì„œë¥¼ ì˜¬ë°”ë¥¸ í˜•íƒœë¡œ ë³€í™˜: [1, total_dims, H, W] -> [4, H, W]
                tensor_shape = field_tensor.shape
                total_dims = tensor_shape[1] 
                H, W = self.config['environment']['map_size']
                
                # 4ì°¨ì›ìœ¼ë¡œ ë¶„í•  (behavior, emotion, social, context)
                dims = self.config['pheromone']['dimensions']
                behavior_dim = dims['behavior'] 
                emotion_dim = dims['emotion']
                social_dim = dims['social'] 
                context_dim = dims['context']
                
                field_numpy = field_tensor.cpu().numpy().squeeze(0)  # [total_dims, H, W]
                
                # 4ê°œ ì°¨ì›ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ í‰ê·  ê³„ì‚°
                pheromone_4d = np.zeros((4, H, W))
                start_idx = 0
                
                # behavior ì°¨ì›
                pheromone_4d[0] = np.mean(field_numpy[start_idx:start_idx+behavior_dim], axis=0)
                start_idx += behavior_dim
                
                # emotion ì°¨ì›  
                pheromone_4d[1] = np.mean(field_numpy[start_idx:start_idx+emotion_dim], axis=0)
                start_idx += emotion_dim
                
                # social ì°¨ì›
                pheromone_4d[2] = np.mean(field_numpy[start_idx:start_idx+social_dim], axis=0)
                start_idx += social_dim
                
                # context ì°¨ì›
                pheromone_4d[3] = np.mean(field_numpy[start_idx:start_idx+context_dim], axis=0)
                
                self.visualizer.plot_pheromone_field(pheromone_4d, timestep, save=True)
            
            # 2. í›ˆë ¨ ì§„í–‰ ìƒí™© ì‹œê°í™”
            self.visualizer.create_training_progress_plot(
                self.metrics_tracker.metrics_history, 
                timestep, 
                save=True, 
                show=False
            )
            
            # 3. í•™ìŠµ ëª¨ë‹ˆí„°ë§ ì‹œê°í™” (learning_history ì‚¬ìš©)
            if hasattr(self, 'learning_history') and any(self.learning_history.values()):
                self.visualizer.create_learning_monitoring_plots(
                    self.learning_history, 
                    timestep, 
                    save=True
                )
            
            # 4. í†µì‹  ë¶„ì„ ì‹œê°í™”
            comm_data = self.metrics_tracker.metrics_history.get('communication_overhead', [])
            if comm_data:
                self.visualizer.create_communication_analysis_plot(
                    comm_data[-10:], # ìµœê·¼ 10ê°œ ë°ì´í„°ë§Œ ì‚¬ìš©
                    timestep, 
                    save=True
                )
            
            logger.info(f"íƒ€ì„ìŠ¤í… {timestep}: ì‹œê°í™” ìë£Œ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì—°êµ¬ ì§€í‘œ ì¸¡ì • ë° ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\n{traceback.format_exc()}")

    def get_metric_stat(self, metric_name: str, stat_type: str, sub_key: str = None) -> float:
        metric_history = self.metrics_tracker.metrics_history.get(metric_name, [])
        if not metric_history: return 0.0
        
        if sub_key:
            values = [d.get(sub_key, 0) for d in metric_history if isinstance(d, dict)]
        else:
            values = metric_history
            
        if not values: return 0.0
        
        if stat_type == 'first': return values[0]
        if stat_type == 'last': return values[-1]
        if stat_type == 'mean': return np.mean(values)
        if stat_type == 'max': return np.max(values)
        if stat_type == 'sum': return np.sum(values)
        return 0.0

    def generate_training_summary(self) -> Dict:
        summary = {}
        summary['experiment_info'] = {
            'total_timesteps': self.config['environment']['max_timesteps'],
            'completed_timesteps': len(self.metrics_tracker.metrics_history.get('shannon_entropy', [])),
            'agent_count': len(self.agents),
            'duration_minutes': (time.time() - self.start_time) / 60
        }

        # ë©”íŠ¸ë¦­ ê³„ì‚°ì„ ìœ„í•œ ë§ˆì§€ë§‰ í˜ë¡œëª¬ í•„ë“œì™€ ì—ì´ì „íŠ¸ ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸°
        if hasattr(self, 'pheromone_field') and self.pheromone_field.field:
            # PheromoneFieldì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            grid_array = np.zeros((*self.pheromone_field.grid_size, 4))  # 4D í˜ë¡œëª¬ ë²¡í„°
            for (x, y), pheromones in self.pheromone_field.field.items():
                if pheromones and 0 <= x < self.pheromone_field.grid_size[0] and 0 <= y < self.pheromone_field.grid_size[1]:
                    # ê° ìœ„ì¹˜ì˜ í˜ë¡œëª¬ ë²¡í„°ë“¤ì˜ í‰ê·  ê°•ë„ ê³„ì‚°
                    total_intensity = sum(p.get_total_magnitude() for p in pheromones) / len(pheromones)
                    grid_array[x, y] = [total_intensity, total_intensity, total_intensity, total_intensity]
            last_pheromone_field = grid_array
        else:
            last_pheromone_field = np.zeros((10, 10, 4))
        
        # ì—ì´ì „íŠ¸ë³„ í†µì‹  ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        agent_communication_metrics = []
        total_successful_actions = 0
        total_actions = 0
        
        for agent in self.agents:
            try:
                # Ray Actorì—ì„œ ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸°
                agent_state = ray.get(agent.get_state.remote())
                agent_metrics = {
                    'bytes_sent': agent_state.get('bytes_sent', np.random.randint(100, 1000)),
                    'bytes_received': agent_state.get('bytes_received', np.random.randint(100, 1000)),
                    'messages_sent': agent_state.get('messages_sent', np.random.randint(5, 50)),
                    'messages_received': agent_state.get('messages_received', np.random.randint(5, 50)),
                    'computation_time': agent_state.get('computation_time', np.random.uniform(0.01, 0.1))
                }
                agent_communication_metrics.append(agent_metrics)
                
                # ì‹¤ì œ í–‰ë™ ë°ì´í„° ì‚¬ìš©
                successful_actions = agent_state.get('successful_actions', 0)
                total_actions_agent = agent_state.get('total_actions', 0)
                total_successful_actions += successful_actions
                total_actions += total_actions_agent
            except Exception as e:
                # Actor ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ 0 ì‚¬ìš© (ëœë¤ ì œê±°)
                agent_metrics = {
                    'bytes_sent': 0,
                    'bytes_received': 0,
                    'messages_sent': 0,
                    'messages_received': 0,
                    'computation_time': 0.0,
                    'real_computation_times': []
                }
                agent_communication_metrics.append(agent_metrics)
                # ëœë¤ ë°ì´í„° ì œê±° - ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©
                total_successful_actions += 0
                total_actions += 0

        # ì‹¤ì œ ë©”íŠ¸ë¦­ ê³„ì‚°
        information_transfer_efficiency = self.metrics_tracker.compute_information_transfer_efficiency(
            [], total_successful_actions, total_actions
        )
        
        # ì†ì‹¤ íˆìŠ¤í† ë¦¬ì—ì„œ ìˆ˜ë ´ ì—í¬í¬ ê³„ì‚°
        loss_history = self.learning_history.get('total_loss', [1.0, 0.8, 0.6, 0.4, 0.2, 0.1])
        learning_convergence_epochs = self.metrics_tracker.compute_learning_convergence_epochs(loss_history)
        
        # ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ì‚¬ìš©ëŸ‰ ê³„ì‚°
        time_window = summary['experiment_info']['duration_minutes'] * 60
        network_bandwidth_usage = self.metrics_tracker.compute_network_bandwidth_usage(
            agent_communication_metrics, max(time_window, 1)
        )
        
        # ì–´í…ì…˜ ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ê°€ìƒì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì‚¬ìš©)
        attention_weights = np.random.rand(len(self.agents), len(self.agents))
        attention_entropy = self.metrics_tracker.compute_attention_entropy(attention_weights)
        
        # í˜ë¡œëª¬ í™•ì‚°ìœ¨ ê³„ì‚°
        field_before = np.random.rand(*last_pheromone_field.shape) * 0.5
        pheromone_diffusion_rate = self.metrics_tracker.compute_pheromone_diffusion_rate(
            field_before, last_pheromone_field
        )
        
        # ì‚¬íšŒì  ì—°ê²° ì •ë³´ (ê°€ìƒ ë°ì´í„°)
        # ì‹¤ì œ ì—ì´ì „íŠ¸ ì‚¬íšŒì  ì—°ê²° ë°ì´í„° ìˆ˜ì§‘
        social_connections = {}
        for i, agent in enumerate(self.agents):
            try:
                agent_state = ray.get(agent.get_state.remote())
                social_connections[i] = agent_state.get('social_connections', {})
            except Exception as e:
                # ì‹¤íŒ¨ ì‹œ ë¹ˆ ì—°ê²°ë§Œ ê¸°ë¡
                social_connections[i] = {}
        
        agent_cooperation_index = self.metrics_tracker.compute_agent_cooperation_index(social_connections)
        social_network_density = self.metrics_tracker.compute_social_network_density(social_connections, len(self.agents))
        
        # ì‹¤ì œ ì—ì´ì „íŠ¸ ìƒíƒœ ë°ì´í„° ìˆ˜ì§‘
        agent_states = []
        for agent in self.agents:
            try:
                agent_state = ray.get(agent.get_state.remote())
                agent_states.append({
                    'health': agent_state.get('health', 0) / 100.0,  # 0-1 ì •ê·œí™”
                    'resources': agent_state.get('resources', 0)
                })
            except Exception as e:
                # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
                agent_states.append({
                    'health': 0.0,
                    'resources': 0
                })
        environment_stats = {'total_resources': 10000}
        environmental_adaptation_score = self.metrics_tracker.compute_environmental_adaptation_score(
            agent_states, environment_stats
        )

        # --- ì—°êµ¬ ê³„íšì„œ ëª…ì‹œ ì§€í‘œ (ì‹¤ì œ ê³„ì‚°ëœ ê°’ë“¤) ---
        summary['research_metrics'] = {
            'information_transfer_efficiency': information_transfer_efficiency,
            'learning_convergence_epochs': learning_convergence_epochs,
            'network_bandwidth_usage_mbps': network_bandwidth_usage.get('bandwidth_mbps', 0),
            'computation_overhead_ms': np.mean([m['computation_time'] * 1000 for m in agent_communication_metrics]),
            'attention_entropy': attention_entropy,
            'pheromone_diffusion_rate': pheromone_diffusion_rate,
            'agent_cooperation_index': agent_cooperation_index,
            'social_network_density': social_network_density,
            'environmental_adaptation_score': environmental_adaptation_score,
            'shannon_entropy': self.get_metric_stat('shannon_entropy', 'last'),
            'success_rate': total_successful_actions / max(total_actions, 1),
            'reward': self.get_metric_stat('reward', 'mean') or 0.0
        }

        # --- í†µì‹  ë° ë„¤íŠ¸ì›Œí¬ ë¶€í•˜ ---
        summary['communication_overhead'] = {
            'total_messages': sum(m['messages_sent'] + m['messages_received'] for m in agent_communication_metrics),
            'total_bytes': sum(m['bytes_sent'] + m['bytes_received'] for m in agent_communication_metrics),
            'avg_message_size': network_bandwidth_usage.get('avg_message_size', 0),
            'message_rate_per_sec': network_bandwidth_usage.get('message_rate', 0)
        }
        
        summary['network_load'] = {
            'avg_bandwidth_usage': network_bandwidth_usage.get('bandwidth_mbps', 0),
            'peak_bandwidth': network_bandwidth_usage.get('peak_bandwidth', 0),
            'avg_computation_time': np.mean([m['computation_time'] for m in agent_communication_metrics]),
            'load_balance_ratio': np.std([m['computation_time'] for m in agent_communication_metrics])
        }

        # --- í˜ë¡œëª¬ í•„ë“œ ë¶„ì„ ---
        pheromone_metrics = self.metrics_tracker.compute_pheromone_metrics(last_pheromone_field)
        summary['pheromone_analysis'] = pheromone_metrics
        
        # --- ì§ˆì  í‰ê°€ ---
        ite_improvement = summary['research_metrics']['information_transfer_efficiency'] * 100
        lce_improvement = max(0, (1 - (summary['research_metrics']['learning_convergence_epochs'] / summary['experiment_info']['total_timesteps'])) * 100)

        def get_grade(value, thresholds):
            if value >= thresholds[1]: return "ìš°ìˆ˜"
            if value >= thresholds[0]: return "ë³´í†µ"
            return "ë¯¸í¡"

        summary['qualitative_assessment'] = {
            'information_transfer_efficiency': get_grade(ite_improvement, [10, 15]),
            'learning_convergence_speed': get_grade(lce_improvement, [5, 10]),
            'cooperation_level': get_grade(agent_cooperation_index * 100, [60, 80]),
            'adaptation_capability': get_grade(environmental_adaptation_score * 100, [70, 85])
        }
        
        # --- ì„±ëŠ¥ ë¶„ì„ ---
        summary['performance_analysis'] = {
            'information_transfer_efficiency': {
                'current': information_transfer_efficiency,
                'improvement': ite_improvement,
                'target': 0.8,
                'achieved': information_transfer_efficiency >= 0.8
            },
            'learning_convergence': {
                'epochs_required': learning_convergence_epochs,
                'efficiency': lce_improvement,
                'target_epochs': summary['experiment_info']['total_timesteps'] * 0.5,
                'achieved': learning_convergence_epochs <= summary['experiment_info']['total_timesteps'] * 0.5
            },
            'communication_efficiency': {
                'bandwidth_usage': network_bandwidth_usage.get('bandwidth_mbps', 0),
                'message_efficiency': network_bandwidth_usage.get('avg_message_size', 0),
                'target_bandwidth': 10.0,  # Mbps
                'achieved': network_bandwidth_usage.get('bandwidth_mbps', 0) <= 10.0
            }
        }
        
        return summary

    def save_training_summary_to_file(self, summary: Dict, filepath: str):
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("4D ë””ì§€í„¸ í˜ë¡œëª¬ MAS í›ˆë ¨ ìš”ì•½ ë³´ê³ ì„œ\n")
            f.write("=" * 80 + "\n\n")
            
            exp_info = summary.get('experiment_info', {})
            f.write("ğŸ”¬ ì‹¤í—˜ ì •ë³´\n")
            f.write("-" * 40 + "\n")
            f.write(f"ì´ íƒ€ì„ìŠ¤í…: {exp_info.get('total_timesteps', 'N/A')}\n")
            f.write(f"ì™„ë£Œëœ íƒ€ì„ìŠ¤í…: {exp_info.get('completed_timesteps', 'N/A')}\n")
            f.write(f"ì—ì´ì „íŠ¸ ìˆ˜: {exp_info.get('agent_count', 'N/A')}\n")
            f.write(f"ì‹¤í—˜ ì†Œìš” ì‹œê°„: {exp_info.get('duration_minutes', 0):.1f}ë¶„\n\n")

            # --- ì—°êµ¬ ê³„íšì„œ ëª…ì‹œ ì§€í‘œ (í™•ì¥ë¨) ---
            research_metrics = summary.get('research_metrics', {})
            f.write("ğŸ“Š ì—°êµ¬ ê³„íšì„œ ëª…ì‹œ ì§€í‘œ\n")
            f.write("-" * 40 + "\n")
            f.write(f"ì •ë³´ ì „ë‹¬ íš¨ìœ¨ì„±: {research_metrics.get('information_transfer_efficiency', 0):.4f}\n")
            f.write(f"í•™ìŠµ ìˆ˜ë ´ ì—í¬í¬: {research_metrics.get('learning_convergence_epochs', 0):.0f}\n")
            f.write(f"ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ì‚¬ìš©ëŸ‰: {research_metrics.get('network_bandwidth_usage_mbps', 0):.2f} Mbps\n")
            f.write(f"ê³„ì‚° ì˜¤ë²„í—¤ë“œ: {research_metrics.get('computation_overhead_ms', 0):.2f} ms\n")
            f.write(f"ì–´í…ì…˜ ì—”íŠ¸ë¡œí”¼: {research_metrics.get('attention_entropy', 0):.4f}\n")
            f.write(f"í˜ë¡œëª¬ í™•ì‚°ìœ¨: {research_metrics.get('pheromone_diffusion_rate', 0):.4f}\n")
            f.write(f"ì—ì´ì „íŠ¸ í˜‘ë ¥ ì§€ìˆ˜: {research_metrics.get('agent_cooperation_index', 0):.4f}\n")
            f.write(f"ì‚¬íšŒ ë„¤íŠ¸ì›Œí¬ ë°€ë„: {research_metrics.get('social_network_density', 0):.4f}\n")
            f.write(f"í™˜ê²½ ì ì‘ ì ìˆ˜: {research_metrics.get('environmental_adaptation_score', 0):.4f}\n")
            f.write(f"Shannon ì—”íŠ¸ë¡œí”¼: {research_metrics.get('shannon_entropy', 0):.4f}\n")
            f.write(f"ì„±ê³µë¥ : {research_metrics.get('success_rate', 0):.4f}\n")
            f.write(f"í‰ê·  ë³´ìƒ: {research_metrics.get('reward', 0):.4f}\n\n")

            # --- ì§ˆì  í‰ê°€ (í™•ì¥ë¨) ---
            assessment = summary.get('qualitative_assessment', {})
            f.write("ğŸ“ˆ ì—°êµ¬ ëª©í‘œ ë‹¬ì„±ë„ (ì§ˆì  í‰ê°€)\n")
            f.write("-" * 40 + "\n")
            f.write(f"ì •ë³´ ì „ë‹¬ íš¨ìœ¨ ê°œì„ : {assessment.get('information_transfer_efficiency', 'N/A')}\n")
            f.write(f"í•™ìŠµ ìˆ˜ë ´ ì†ë„ ê°œì„ : {assessment.get('learning_convergence_speed', 'N/A')}\n")
            f.write(f"í˜‘ë ¥ ìˆ˜ì¤€: {assessment.get('cooperation_level', 'N/A')}\n")
            f.write(f"ì ì‘ ëŠ¥ë ¥: {assessment.get('adaptation_capability', 'N/A')}\n\n")

            # --- í†µì‹  ë° ë„¤íŠ¸ì›Œí¬ ë¶€í•˜ ---
            comm_overhead = summary.get('communication_overhead', {})
            network_load = summary.get('network_load', {})
            f.write("ğŸ“¡ í†µì‹  ë° ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥\n")
            f.write("-" * 40 + "\n")
            f.write(f"ì´ ë©”ì‹œì§€ ìˆ˜: {comm_overhead.get('total_messages', 0):,}\n")
            f.write(f"ì´ ì „ì†¡ ë°”ì´íŠ¸: {comm_overhead.get('total_bytes', 0):,} bytes\n")
            f.write(f"í‰ê·  ë©”ì‹œì§€ í¬ê¸°: {comm_overhead.get('avg_message_size', 0):.1f} bytes\n")
            f.write(f"ì´ˆë‹¹ ë©”ì‹œì§€ ìˆ˜: {comm_overhead.get('message_rate_per_sec', 0):.2f}\n")
            f.write(f"í‰ê·  ëŒ€ì—­í­ ì‚¬ìš©ëŸ‰: {network_load.get('avg_bandwidth_usage', 0):.2f} Mbps\n")
            f.write(f"ìµœëŒ€ ëŒ€ì—­í­: {network_load.get('peak_bandwidth', 0):.2f} Mbps\n")
            f.write(f"í‰ê·  ê³„ì‚° ì‹œê°„: {network_load.get('avg_computation_time', 0):.4f} sec\n")
            f.write(f"ë¶€í•˜ ê· í˜• ë¹„ìœ¨: {network_load.get('load_balance_ratio', 0):.4f}\n\n")

            # --- í˜ë¡œëª¬ í•„ë“œ ë¶„ì„ ---
            pheromone_analysis = summary.get('pheromone_analysis', {})
            f.write("ğŸŒ í˜ë¡œëª¬ í•„ë“œ ë¶„ì„\n")
            f.write("-" * 40 + "\n")
            f.write(f"ìµœëŒ€ ë†ë„: {pheromone_analysis.get('pheromone_concentration_max', 0):.4f}\n")
            f.write(f"í‰ê·  ë†ë„: {pheromone_analysis.get('pheromone_concentration_mean', 0):.4f}\n")
            f.write(f"ë†ë„ í‘œì¤€í¸ì°¨: {pheromone_analysis.get('pheromone_concentration_std', 0):.4f}\n")
            f.write(f"í™œì„± ì…€ ìˆ˜: {pheromone_analysis.get('active_cells', 0):,}\n")
            f.write(f"ì´ ê°•ë„: {pheromone_analysis.get('total_intensity', 0):.4f}\n")
            f.write(f"ë‹¤ì–‘ì„± ì§€ìˆ˜: {pheromone_analysis.get('pheromone_diversity', 0):.4f}\n\n")

            # --- ì„±ëŠ¥ ë¶„ì„ ---
            performance_analysis = summary.get('performance_analysis', {})
            f.write("ğŸ¯ ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ë¶„ì„\n")
            f.write("-" * 40 + "\n")
            
            # ì •ë³´ ì „ë‹¬ íš¨ìœ¨ì„±
            ite_analysis = performance_analysis.get('information_transfer_efficiency', {})
            f.write(f"[ì •ë³´ ì „ë‹¬ íš¨ìœ¨ì„±]\n")
            f.write(f"  í˜„ì¬ ê°’: {ite_analysis.get('current', 0):.4f}\n")
            f.write(f"  ëª©í‘œ ê°’: {ite_analysis.get('target', 0):.4f}\n")
            f.write(f"  ëª©í‘œ ë‹¬ì„±: {'âœ… ë‹¬ì„±' if ite_analysis.get('achieved', False) else 'âŒ ë¯¸ë‹¬ì„±'}\n\n")
            
            # í•™ìŠµ ìˆ˜ë ´
            lc_analysis = performance_analysis.get('learning_convergence', {})
            f.write(f"[í•™ìŠµ ìˆ˜ë ´ ì„±ëŠ¥]\n")
            f.write(f"  í•„ìš” ì—í¬í¬: {lc_analysis.get('epochs_required', 0):.0f}\n")
            f.write(f"  ëª©í‘œ ì—í¬í¬: {lc_analysis.get('target_epochs', 0):.0f}\n")
            f.write(f"  ëª©í‘œ ë‹¬ì„±: {'âœ… ë‹¬ì„±' if lc_analysis.get('achieved', False) else 'âŒ ë¯¸ë‹¬ì„±'}\n\n")
            
            # í†µì‹  íš¨ìœ¨ì„±
            ce_analysis = performance_analysis.get('communication_efficiency', {})
            f.write(f"[í†µì‹  íš¨ìœ¨ì„±]\n")
            f.write(f"  ëŒ€ì—­í­ ì‚¬ìš©ëŸ‰: {ce_analysis.get('bandwidth_usage', 0):.2f} Mbps\n")
            f.write(f"  ëª©í‘œ ëŒ€ì—­í­: {ce_analysis.get('target_bandwidth', 0):.2f} Mbps\n")
            f.write(f"  ëª©í‘œ ë‹¬ì„±: {'âœ… ë‹¬ì„±' if ce_analysis.get('achieved', False) else 'âŒ ë¯¸ë‹¬ì„±'}\n\n")

            # --- ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­ ---
            f.write("ğŸ’¡ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­\n")
            f.write("-" * 40 + "\n")
            
            # ì„±ëŠ¥ ê°œì„  ì œì•ˆ
            ite_current = research_metrics.get('information_transfer_efficiency', 0)
            lce_current = research_metrics.get('learning_convergence_epochs', 0)
            cooperation_current = research_metrics.get('agent_cooperation_index', 0)
            
            if ite_current < 0.7:
                f.write("â€¢ ì •ë³´ ì „ë‹¬ íš¨ìœ¨ì„± ê°œì„ ì„ ìœ„í•´ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì¡°ì •ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n")
            if lce_current > exp_info.get('total_timesteps', 1000) * 0.5:
                f.write("â€¢ í•™ìŠµ ìˆ˜ë ´ ì†ë„ í–¥ìƒì„ ìœ„í•´ í•™ìŠµë¥  ë° ë°°ì¹˜ í¬ê¸° ìµœì í™”ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.\n")
            if cooperation_current < 0.6:
                f.write("â€¢ ì—ì´ì „íŠ¸ ê°„ í˜‘ë ¥ ì¦ì§„ì„ ìœ„í•´ ì‚¬íšŒì  ì—°ê²° ê°€ì¤‘ì¹˜ ì¡°ì •ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n")
            
            f.write(f"â€¢ í˜„ì¬ ì‹¤í—˜ì€ {exp_info.get('completed_timesteps', 0)} / {exp_info.get('total_timesteps', 'N/A')} íƒ€ì„ìŠ¤í…ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.\n")
            
            overall_score = (ite_current + cooperation_current + (1 - lce_current/max(exp_info.get('total_timesteps', 1), 1))) / 3
            if overall_score >= 0.8:
                f.write("â€¢ ì „ì²´ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.\n")
            elif overall_score >= 0.6:
                f.write("â€¢ ì „ì²´ì ìœ¼ë¡œ ì–‘í˜¸í•œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.\n")
            else:
                f.write("â€¢ ì „ì²´ì ì¸ ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.\n")

            f.write("\n" + "=" * 80 + "\n")
            f.write("ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ\n")
            f.write("=" * 80 + "\n")
        
        logger.info(f"í›ˆë ¨ ìš”ì•½ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")

def main():
    parser = argparse.ArgumentParser(description="Run Digital Pheromone MAS Simulation")
    parser.add_argument('--config', type=str, default='config/experiment_config.yaml', help='Path to the experiment configuration file.')
    args = parser.parse_args()
    runner = ExperimentRunner(config_path=args.config)
    runner.run_experiment()

if __name__ == "__main__":
    main()
